{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Set up IPython to show all outputs from a cell\n",
    "import warnings\n",
    "import numpy as np\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "\n",
    "\n",
    "from numpy import load\n",
    "\n",
    "idx_word = load('idx_word.npy').tolist()\n",
    "\n",
    "\n",
    "\n",
    "from utils_2 import format_text, remove_spaces#, make_sequences, create_train_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "\n",
    "def load_and_evaluate(model_dir1, model_name, return_model=False):\n",
    "    \"\"\"Load in a trained model and evaluate with log loss and accuracy\"\"\"\n",
    "\n",
    "    model = load_model('{}/{}.h5'.format(model_dir1 ,model_name))\n",
    "    #r = model.evaluate(X_valid, y_valid, batch_size=2048, verbose=1)\n",
    "\n",
    "    #valid_crossentropy = r[0]\n",
    "    #valid_accuracy = r[1]\n",
    "\n",
    "    #print('Cross Entropy: {}'.format(round(valid_crossentropy, 4)))\n",
    "    #print('Accuracy: {}%'.format(round(100 * valid_accuracy, 2)))\n",
    "\n",
    "    if return_model:\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing models: \n",
      "models/whitman.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/frost.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/dickinson.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/seuss.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/shakespeare.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['whitman', 'frost', 'dickinson', 'seuss', 'shakespeare']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model_name = 'whitman'\n",
    "model_dir = 'models'\n",
    "import os\n",
    "models = []\n",
    "titles=[]\n",
    "print(\"importing models: \")\n",
    "for file in os.listdir(model_dir):\n",
    "    if file.endswith(\".h5\"):\n",
    "        titles.append(file[:-3])\n",
    "        print(os.path.join(model_dir, file))\n",
    "        model = load_and_evaluate(model_dir, file[:-3], return_model=True)\n",
    "        models.append(model)\n",
    "titles\n",
    "len(models)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "idx_word[1]\n",
    "\n",
    "def stringToInts(input1):\n",
    "    input2 = format_text(input1)\n",
    "    array = input2.split(' ')\n",
    "    #print(array)\n",
    "    output = []\n",
    "    for word in array:\n",
    "        found = False\n",
    "        for x, y in idx_word.items():\n",
    "            if y == word:\n",
    "                output.append(x)\n",
    "                found=True\n",
    "                break\n",
    "        if not found:        \n",
    "            print(str(word) + \" not found in dict\")\n",
    "    return output\n",
    "    \n",
    "#stringToInts(\"walk to the. store yeet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def generate_output(model,\n",
    "                    sequences,\n",
    "                    training_length=50,\n",
    "                    new_words=100,\n",
    "                    diversity=1,\n",
    "                    return_output=False,\n",
    "                    n_gen=1):\n",
    "    \"\"\"Generate `new_words` words of output from a trained model and format into HTML.\"\"\"\n",
    "\n",
    "   \n",
    "\n",
    "    gen_list = []\n",
    "\n",
    "    for n in range(n_gen):\n",
    "        # Extract the seed sequence\n",
    "        seed = stringToInts(sequences)\n",
    "        #seed = [6,179,7,3,230,326,13,2]#,1589,1,247,1,3,109,134,16,1,2,3,73,681,1426,134,16,1,1590,4682,6,4683,13,2,396,6,1179,20,2845,1,6,84,67,2845,1,2,396,6,4684,49,59,1,3556]\n",
    "        n2=[]\n",
    "        for i in seed:\n",
    "            n2.append(idx_word.get(i, '< --- >'))\n",
    "        \n",
    "        #seq[seed_idx:end_idx]\n",
    "        #original_sequence = [idx_word[i] for i in seed]\n",
    "        generated = seed[:] + ['#']\n",
    "\n",
    "        # Find the actual entire sequence\n",
    "        actual = generated[:]\n",
    "        output = []\n",
    "        # Keep adding new words\n",
    "        for i in range(new_words):\n",
    "\n",
    "            # Make a prediction from the seed\n",
    "            preds = model.predict(np.array(seed).reshape(1, -1))[0].astype(\n",
    "                np.float64)\n",
    "\n",
    "            # Diversify\n",
    "            preds = np.log(preds) / diversity\n",
    "            exp_preds = np.exp(preds)\n",
    "\n",
    "            # Softmax\n",
    "            preds = exp_preds / sum(exp_preds)\n",
    "\n",
    "            \n",
    "            \n",
    "            # Choose the next word\n",
    "            probas = np.random.multinomial(1, preds, 1)[0]\n",
    "\n",
    "            next_idx = np.argmax(probas)\n",
    "\n",
    "            # New seed adds on old word\n",
    "            seed = seed[1:] + [next_idx]\n",
    "            generated.append(next_idx)\n",
    "            output.append(next_idx)\n",
    "\n",
    "        # Showing generated and actual abstract\n",
    "        n = []\n",
    "\n",
    "        for i in output:\n",
    "            n.append(idx_word.get(i, '< --- >'))\n",
    "\n",
    "        gen_list.append(n)\n",
    "        #print(gen_list)\n",
    "        \n",
    "        \n",
    "\n",
    "        #print(\"original\")\n",
    "        #print(remove_spaces(' '.join(n2)))\n",
    "        #print(\"continuation:\")\n",
    "        #print(n)\n",
    "        \n",
    "    a = []\n",
    "\n",
    "    gen_list = [\n",
    "        gen[training_length:training_length + len(a)] for gen in gen_list\n",
    "    ]\n",
    "\n",
    "    if return_output:\n",
    "        return original_sequence, gen_list, a\n",
    "\n",
    "    # HTML formatting\n",
    "    seed_html = ''\n",
    "    seed_html = addContent(seed_html, header(\n",
    "        'Seed Sequence', color='darkblue'))\n",
    "    seed_html = addContent(seed_html,\n",
    "                           box(remove_spaces(' '.join(\"original_sequence\"))))\n",
    "\n",
    "    gen_html = ''\n",
    "    gen_html = addContent(gen_html, header('RNN Generated', color='darkred'))\n",
    "    print(remove_spaces(' '.join(n)))\n",
    "    gen_html = addContent(gen_html, box(remove_spaces(' '.join(n))))\n",
    "\n",
    "    return seed_html, gen_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "','"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def alliterate(word, prevWords):\n",
    "    for otherword in prevWords:\n",
    "        if word[0] == otherword[0]:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def rhymeswith(word, prevWords):\n",
    "    return 0\n",
    "\n",
    "def mesh_authors(model, author1, author2, \n",
    "                    sequences,\n",
    "                    training_length=50,\n",
    "                    new_words=100,\n",
    "                    diversity=1,\n",
    "                    return_output=False,\n",
    "                    n_gen=1, auth1WEIGHT = .5):\n",
    "\n",
    "    gen_list = []\n",
    "    output_text = []\n",
    "    print(\"generating output from \" + titles[author1]+\" and \"+ titles[author2])\n",
    "    for n in range(n_gen):\n",
    "        # Extract the seed sequence\n",
    "        seed = stringToInts(sequences)\n",
    "        comprehended_words=[]\n",
    "        for i in seed:\n",
    "            comprehended_words.append(idx_word.get(i, '< --- >'))\n",
    "        \n",
    "        generated = seed[:] + ['#']\n",
    "        # Find the actual entire sequence\n",
    "        actual = generated[:]\n",
    "        output = []\n",
    "        # Keep adding new words\n",
    "        for i in range(new_words):\n",
    "            \n",
    "            # predict next word for author 1\n",
    "            pred1 = model[author1].predict(np.array(seed).reshape(1, -1))[0].astype(\n",
    "                np.float64)\n",
    "            # predict next word for author 2\n",
    "            pred2 = model[author2].predict(np.array(seed).reshape(1, -1))[0].astype(\n",
    "                np.float64)\n",
    "            \n",
    "                \n",
    "            # Diversify\n",
    "            pred1 = np.log(pred1) / diversity\n",
    "            exp_preds = np.exp(pred1)\n",
    "            # Softmax\n",
    "            pred1 = exp_preds / sum(exp_preds)\n",
    "\n",
    "            pred2 = np.log(pred2) / diversity\n",
    "            exp_preds = np.exp(pred2)\n",
    "            # Softmax\n",
    "            pred2 = exp_preds / sum(exp_preds)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #combine probability vectors\n",
    "            pred1 = pred1 * auth1WEIGHT\n",
    "            pred2 = pred2 * (1-auth1WEIGHT)\n",
    "            \n",
    "            preds = pred1 + pred2\n",
    "            \n",
    "            \n",
    "            for pos in range(1,len(idx_word)):\n",
    "                word = idx_word[pos]\n",
    "                if rhymeswith(word, output_text[-40:]):\n",
    "                    preds[pos] *=3;\n",
    "                if alliterate(word, output_text[-2:]):\n",
    "                    preds[pos] *=5;\n",
    "            preds=preds/np.sum(preds)\n",
    "            \n",
    "            # Choose the next word\n",
    "            probas = np.random.multinomial(1, preds, 1)[0]\n",
    "            next_idx = np.argmax(probas)\n",
    "\n",
    "            # New seed adds on old word\n",
    "            seed = seed[1:] + [next_idx]\n",
    "            generated.append(next_idx)\n",
    "            output.append(next_idx)\n",
    "            output_text.append(idx_word.get(next_idx, '< --- >'))\n",
    "        # Showing generated and actual abstract\n",
    "        \n",
    "           \n",
    "            \n",
    "    return remove_spaces(' '.join(output_text))\n",
    "idx_word[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whitman: 0\n",
      "frost: 1\n",
      "dickinson: 2\n",
      "seuss: 3\n",
      "shakespeare: 4\n",
      "generating output from whitman and dickinson\n",
      "ballooning treasure tour defies d ga-zump dead-house launchest cities yellowstone cambodia snowy crevice tenons temple fulfill'd taste flatting themselves australia the any rest removed reverent artisans augmented snow-bank sluttish swerve clomping sarcastically sparkles weave dinner-table dissolute disappear'd ministers nativity stemming hegel several anchor selling sisters containing charging cheerfulest cherish'd juniper-tree\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(titles)):\n",
    "    print(titles[i] + \": \" + str(i))\n",
    "    \n",
    "print(mesh_authors(models, 0, , \"test\", new_words=50,diversity=1,auth1WEIGHT=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whitman:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'addContent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a27544777679>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\":\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mseed_html\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_html\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"i ran away from you\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdiversity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#HTML(seed_html)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-5eb8a69aec19>\u001b[0m in \u001b[0;36mgenerate_output\u001b[0;34m(model, sequences, training_length, new_words, diversity, return_output, n_gen)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# HTML formatting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mseed_html\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     seed_html = addContent(seed_html, header(\n\u001b[0m\u001b[1;32m     86\u001b[0m         'Seed Sequence', color='darkblue'))\n\u001b[1;32m     87\u001b[0m     seed_html = addContent(seed_html,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'addContent' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(titles)):\n",
    "    print(titles[i] + \":\")\n",
    "    seed_html, gen_html = generate_output(models[i], \"i ran away from you\", 10,diversity=1)\n",
    "    print(\"\\n\\n\\n\")\n",
    "#HTML(seed_html)\n",
    "#HTML(gen_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
