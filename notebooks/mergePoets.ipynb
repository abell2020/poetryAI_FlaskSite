{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Set up IPython to show all outputs from a cell\n",
    "import warnings\n",
    "import numpy as np\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "RANDOM_STATE = 50\n",
    "EPOCHS = 150\n",
    "BATCH_SIZE = 2048\n",
    "TRAINING_LENGTH = 15\n",
    "TRAIN_FRACTION = 0.7\n",
    "LSTM_CELLS = 64\n",
    "VERBOSE = 0\n",
    "SAVE_MODEL = True\n",
    "\n",
    "\n",
    "\n",
    "from utils_2 import format_text, remove_spaces, make_sequences, create_train_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.utils import plot_model\n",
    "\n",
    "def make_word_level_model(num_words,\n",
    "                          embedding_matrix,\n",
    "                          lstm_cells=64,\n",
    "                          trainable=False,\n",
    "                          lstm_layers=1,\n",
    "                          bi_direc=False):\n",
    "    \"\"\"Make a word level recurrent neural network with option for pretrained embeddings\n",
    "       and varying numbers of LSTM cell layers.\"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Map words to an embedding\n",
    "    if not trainable:\n",
    "        model.add(\n",
    "            Embedding(\n",
    "                input_dim=num_words,\n",
    "                output_dim=embedding_matrix.shape[1],\n",
    "                weights=[embedding_matrix],\n",
    "                trainable=False,\n",
    "                mask_zero=True))\n",
    "        model.add(Masking())\n",
    "    else:\n",
    "        model.add(\n",
    "            Embedding(\n",
    "                input_dim=num_words,\n",
    "                output_dim=embedding_matrix.shape[1],\n",
    "                weights=[embedding_matrix],\n",
    "                trainable=True))\n",
    "\n",
    "    # If want to add multiple LSTM layers\n",
    "    if lstm_layers > 1:\n",
    "        for i in range(lstm_layers - 1):\n",
    "            model.add(\n",
    "                LSTM(\n",
    "                    lstm_cells,\n",
    "                    return_sequences=True,\n",
    "                    dropout=0.1,\n",
    "                    recurrent_dropout=0.1))\n",
    "\n",
    "    # Add final LSTM cell layer\n",
    "    if bi_direc:\n",
    "        model.add(\n",
    "            Bidirectional(\n",
    "                LSTM(\n",
    "                    lstm_cells,\n",
    "                    return_sequences=False,\n",
    "                    dropout=0.1,\n",
    "                    recurrent_dropout=0.1)))\n",
    "    else:\n",
    "        model.add(\n",
    "            LSTM(\n",
    "                lstm_cells,\n",
    "                return_sequences=False,\n",
    "                dropout=0.1,\n",
    "                recurrent_dropout=0.1))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    # Dropout for regularization\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(num_words, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def make_callbacks(model_name, save=SAVE_MODEL):\n",
    "    \"\"\"Make list of callbacks for training\"\"\"\n",
    "\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=40)]\n",
    "        #print(\"callback\")\n",
    "    if save:\n",
    "          callbacks.append(\n",
    "             ModelCheckpoint('{}{}.h5'.format(model_dir,model_name),\n",
    "                save_best_only=True,\n",
    "                save_weights_only=False))\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_answer(index):\n",
    "    \"\"\"Find label corresponding to features for index in training data\"\"\"\n",
    "\n",
    "    # Find features and label\n",
    "    feats = ' '.join(idx_word[i] for i in features[index])\n",
    "    answer = idx_word[labels[index]]\n",
    "\n",
    "    print('Features:', feats)\n",
    "    print('\\nLabel: ', answer)\n",
    "    \n",
    "#find_answer(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data from: \n",
      "data/dickinson.txt\n",
      "data/shakespeare.txt\n",
      "data/seuss.txt\n",
      "data/frost.txt\n",
      "data/whitman.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['dickinson', 'shakespeare', 'seuss', 'frost', 'whitman']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "formatted = []\n",
    "titles=[]\n",
    "print(\"reading data from: \")\n",
    "for file in os.listdir(\"data\"):\n",
    "    if file.endswith(\".txt\"):\n",
    "        titles.append(file[:-4])\n",
    "        print(os.path.join(\"data\", file))\n",
    "        f = open(os.path.join(\"data\", file), \"r\")\n",
    "        raw=f.read()\n",
    "        formatted.append(format_text(raw))\n",
    "titles\n",
    "len(formatted)\n",
    "#formatted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dictionary size: \n",
      "19113\n",
      "trainingseqLength: \n",
      "38852\n"
     ]
    }
   ],
   "source": [
    "filters = '%[\\\\]^_`{|}~\\t'\n",
    "word_idx, idx_word, num_words, word_counts, abstracts, sequences, features, labels = make_sequences(formatted, TRAINING_LENGTH, lower=True)\n",
    "\n",
    "from numpy import save\n",
    "save('idx_word.npy', idx_word)\n",
    "#word_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 4294 words without pre-trained embeddings.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import stanford pre-trained word association vectors\n",
    "import os\n",
    "from keras.utils import get_file\n",
    "import numpy as np\n",
    "\n",
    "glove_vectors = 'glove.6B.100d.txt'\n",
    "glove = np.loadtxt(glove_vectors, dtype='str', comments=None)\n",
    "#glove.shape\n",
    "\n",
    "\n",
    "vectors = glove[:, 1:].astype('float')\n",
    "words = glove[:, 0]\n",
    "\n",
    "del glove\n",
    "\n",
    "#vectors[100], words[100]\n",
    "\n",
    "word_lookup = {word: vector for word, vector in zip(words, vectors)}\n",
    "\n",
    "embedding_matrix = np.zeros((num_words, vectors.shape[1]))\n",
    "\n",
    "not_found = 0\n",
    "\n",
    "for i in range(1,len(idx_word)):\n",
    "    # Look up the word embedding\n",
    "    word=idx_word[i]\n",
    "    vector = word_lookup.get(word, None)\n",
    "\n",
    "    # Record in matrix\n",
    "    if vector is not None:\n",
    "        #print(word)\n",
    "        embedding_matrix[i, :] = vector\n",
    "    else:\n",
    "        not_found += 1\n",
    "\n",
    "print('There were {} words without pre-trained embeddings.'.format(not_found))\n",
    "\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "del vectors\n",
    "gc.collect()\n",
    "\n",
    "# Normalize and convert nan to 0\n",
    "embedding_matrix = embedding_matrix / \\\n",
    "    np.linalg.norm(embedding_matrix, axis=1).reshape((-1, 1))\n",
    "embedding_matrix = np.nan_to_num(embedding_matrix)\n",
    "\n",
    "def find_closest(query, embedding_matrix, word_idx, idx_word, n=10):\n",
    "    \"\"\"Find closest words to a query word in embeddings\"\"\"\n",
    "\n",
    "    idx = word_idx.get(query, None)\n",
    "    # Handle case where query is not in vocab\n",
    "    if idx is None:\n",
    "        print('{query} not found in vocab.'.format(query=query))\n",
    "        return\n",
    "    else:\n",
    "        vec = embedding_matrix[idx]\n",
    "        # Handle case where word doesn't have an embedding\n",
    "        if np.all(vec == 0):\n",
    "            print('{query} has no pre-trained embedding.'.format(query=query))\n",
    "            return\n",
    "        else:\n",
    "            # Calculate distance between vector and all others\n",
    "            dists = np.dot(embedding_matrix, vec)\n",
    "\n",
    "            # Sort indexes in reverse order\n",
    "            idxs = np.argsort(dists)[::-1][:n]\n",
    "            sorted_dists = dists[idxs]\n",
    "            closest = [idx_word[i] for i in idxs]\n",
    "\n",
    "    print('Query: {query}\\n'.format(query=query))\n",
    "    max_len = max([len(i) for i in closest])\n",
    "    # Print out the word and cosine distances\n",
    "    for word, dist in zip(closest, sorted_dists):\n",
    "        print('Word: {wor} Cosine Similarity: {sec}'.format(wor=word[:15], sec=round(dist, 4)))\n",
    "        \n",
    "#find_closest('swamp', embedding_matrix, word_idx, idx_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating training data for dickinson\n",
      "dictionary size: \n",
      "19113\n",
      "trainingseqLength: \n",
      "38852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(27196, 15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(27196, 19113)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object: y_valid\n",
      "\tSize (GB):  0.22278124\n",
      "Object: y_train\n",
      "\tSize (GB):  0.51979726\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 100)         1911300   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 19113)             2465577   \n",
      "=================================================================\n",
      "Total params: 4,427,437\n",
      "Trainable params: 4,427,437\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27196 samples, validate on 11656 samples\n",
      "Epoch 1/1000\n",
      "27196/27196 [==============================] - 6s 203us/step - loss: 9.7928 - accuracy: 0.0596 - val_loss: 9.4749 - val_accuracy: 0.1401\n",
      "Epoch 2/1000\n",
      "27196/27196 [==============================] - 4s 151us/step - loss: 8.1485 - accuracy: 0.1017 - val_loss: 6.4127 - val_accuracy: 0.1401\n",
      "Epoch 3/1000\n",
      "27196/27196 [==============================] - 4s 155us/step - loss: 6.1424 - accuracy: 0.1387 - val_loss: 6.3120 - val_accuracy: 0.1401\n",
      "Epoch 4/1000\n",
      "27196/27196 [==============================] - 4s 154us/step - loss: 5.9275 - accuracy: 0.1404 - val_loss: 6.1839 - val_accuracy: 0.1401\n",
      "Epoch 5/1000\n",
      "27196/27196 [==============================] - 4s 149us/step - loss: 5.8534 - accuracy: 0.1413 - val_loss: 6.1677 - val_accuracy: 0.1401\n",
      "Epoch 6/1000\n",
      "27196/27196 [==============================] - 4s 148us/step - loss: 5.8204 - accuracy: 0.1411 - val_loss: 6.1423 - val_accuracy: 0.1401\n",
      "Epoch 7/1000\n",
      "27196/27196 [==============================] - 4s 149us/step - loss: 5.7918 - accuracy: 0.1413 - val_loss: 6.1391 - val_accuracy: 0.1401\n",
      "Epoch 8/1000\n",
      "27196/27196 [==============================] - 4s 149us/step - loss: 5.7563 - accuracy: 0.1411 - val_loss: 6.1147 - val_accuracy: 0.1401\n",
      "Epoch 9/1000\n",
      "27196/27196 [==============================] - 4s 149us/step - loss: 5.7087 - accuracy: 0.1412 - val_loss: 6.0843 - val_accuracy: 0.1401\n",
      "Epoch 10/1000\n",
      "27196/27196 [==============================] - 4s 149us/step - loss: 5.6556 - accuracy: 0.1412 - val_loss: 6.0383 - val_accuracy: 0.1401\n",
      "Epoch 11/1000\n",
      "27196/27196 [==============================] - 4s 148us/step - loss: 5.6017 - accuracy: 0.1412 - val_loss: 6.0139 - val_accuracy: 0.1401\n",
      "Epoch 12/1000\n",
      "27196/27196 [==============================] - 4s 148us/step - loss: 5.5625 - accuracy: 0.1412 - val_loss: 6.0081 - val_accuracy: 0.1401\n",
      "Epoch 13/1000\n",
      "27196/27196 [==============================] - 4s 147us/step - loss: 5.5363 - accuracy: 0.1412 - val_loss: 6.0209 - val_accuracy: 0.1401\n",
      "Epoch 14/1000\n",
      "27196/27196 [==============================] - 4s 148us/step - loss: 5.5132 - accuracy: 0.1412 - val_loss: 6.0345 - val_accuracy: 0.1401\n",
      "Epoch 15/1000\n",
      "27196/27196 [==============================] - 4s 149us/step - loss: 5.4964 - accuracy: 0.1412 - val_loss: 6.0528 - val_accuracy: 0.1401\n",
      "Epoch 16/1000\n",
      "27196/27196 [==============================] - 4s 148us/step - loss: 5.4842 - accuracy: 0.1412 - val_loss: 6.0676 - val_accuracy: 0.1401\n",
      "Epoch 17/1000\n",
      "27196/27196 [==============================] - 4s 148us/step - loss: 5.4697 - accuracy: 0.1412 - val_loss: 6.0770 - val_accuracy: 0.1401\n",
      "Epoch 18/1000\n",
      "27196/27196 [==============================] - 4s 148us/step - loss: 5.4586 - accuracy: 0.1412 - val_loss: 6.0979 - val_accuracy: 0.1401\n",
      "Epoch 19/1000\n",
      "27196/27196 [==============================] - 4s 148us/step - loss: 5.4461 - accuracy: 0.1415 - val_loss: 6.1039 - val_accuracy: 0.1400\n",
      "Epoch 20/1000\n",
      "27196/27196 [==============================] - 4s 148us/step - loss: 5.4344 - accuracy: 0.1429 - val_loss: 6.1126 - val_accuracy: 0.1400\n",
      "Epoch 21/1000\n",
      "27196/27196 [==============================] - 4s 148us/step - loss: 5.4222 - accuracy: 0.1479 - val_loss: 6.1168 - val_accuracy: 0.1458\n",
      "Epoch 22/1000\n",
      "27196/27196 [==============================] - 4s 148us/step - loss: 5.4114 - accuracy: 0.1553 - val_loss: 6.1262 - val_accuracy: 0.1597\n",
      "Epoch 23/1000\n",
      "27196/27196 [==============================] - 4s 148us/step - loss: 5.3905 - accuracy: 0.1577 - val_loss: 6.1371 - val_accuracy: 0.1621\n",
      "Epoch 24/1000\n",
      "27196/27196 [==============================] - 4s 147us/step - loss: 5.3784 - accuracy: 0.1612 - val_loss: 6.1350 - val_accuracy: 0.1623\n",
      "Epoch 25/1000\n",
      "27196/27196 [==============================] - 4s 148us/step - loss: 5.3711 - accuracy: 0.1631 - val_loss: 6.1508 - val_accuracy: 0.1627\n",
      "Epoch 26/1000\n",
      "27196/27196 [==============================] - 4s 149us/step - loss: 5.3602 - accuracy: 0.1631 - val_loss: 6.1497 - val_accuracy: 0.1630\n",
      "Epoch 27/1000\n",
      "27196/27196 [==============================] - 4s 148us/step - loss: 5.3452 - accuracy: 0.1680 - val_loss: 6.1579 - val_accuracy: 0.1630\n",
      "Epoch 28/1000\n",
      "27196/27196 [==============================] - 4s 148us/step - loss: 5.3406 - accuracy: 0.1693 - val_loss: 6.1745 - val_accuracy: 0.1659\n",
      "Epoch 29/1000\n",
      "27196/27196 [==============================] - 4s 146us/step - loss: 5.3233 - accuracy: 0.1705 - val_loss: 6.1788 - val_accuracy: 0.1709\n",
      "Epoch 30/1000\n",
      "27196/27196 [==============================] - 4s 148us/step - loss: 5.3111 - accuracy: 0.1733 - val_loss: 6.1843 - val_accuracy: 0.1694\n",
      "Epoch 31/1000\n",
      "27196/27196 [==============================] - 4s 149us/step - loss: 5.3009 - accuracy: 0.1750 - val_loss: 6.1857 - val_accuracy: 0.1683\n",
      "Epoch 32/1000\n",
      "27196/27196 [==============================] - 4s 148us/step - loss: 5.2893 - accuracy: 0.1760 - val_loss: 6.1926 - val_accuracy: 0.1695\n",
      "Epoch 33/1000\n",
      "27196/27196 [==============================] - 4s 148us/step - loss: 5.2721 - accuracy: 0.1784 - val_loss: 6.1944 - val_accuracy: 0.1738\n",
      "Epoch 34/1000\n",
      "27196/27196 [==============================] - 4s 148us/step - loss: 5.2585 - accuracy: 0.1819 - val_loss: 6.2023 - val_accuracy: 0.1745\n",
      "Epoch 35/1000\n",
      "27196/27196 [==============================] - 4s 149us/step - loss: 5.2440 - accuracy: 0.1823 - val_loss: 6.1988 - val_accuracy: 0.1776\n",
      "Epoch 36/1000\n",
      "27196/27196 [==============================] - 4s 148us/step - loss: 5.2208 - accuracy: 0.1871 - val_loss: 6.1970 - val_accuracy: 0.1772\n",
      "Epoch 37/1000\n",
      "27196/27196 [==============================] - 4s 148us/step - loss: 5.2051 - accuracy: 0.1895 - val_loss: 6.1909 - val_accuracy: 0.1798\n",
      "Epoch 38/1000\n",
      "27196/27196 [==============================] - 4s 148us/step - loss: 5.1754 - accuracy: 0.1924 - val_loss: 6.1806 - val_accuracy: 0.1815\n",
      "Epoch 39/1000\n",
      "27196/27196 [==============================] - 4s 148us/step - loss: 5.1452 - accuracy: 0.1964 - val_loss: 6.1708 - val_accuracy: 0.1854\n",
      "Epoch 40/1000\n",
      "27196/27196 [==============================] - 4s 148us/step - loss: 5.1182 - accuracy: 0.1998 - val_loss: 6.1617 - val_accuracy: 0.1867\n",
      "Epoch 41/1000\n",
      "27196/27196 [==============================] - 4s 148us/step - loss: 5.0845 - accuracy: 0.2048 - val_loss: 6.1519 - val_accuracy: 0.1882\n",
      "Epoch 42/1000\n",
      "27196/27196 [==============================] - 4s 149us/step - loss: 5.0583 - accuracy: 0.2084 - val_loss: 6.1455 - val_accuracy: 0.1920\n",
      "Epoch 43/1000\n",
      "27196/27196 [==============================] - 4s 151us/step - loss: 5.0345 - accuracy: 0.2102 - val_loss: 6.1461 - val_accuracy: 0.1927\n",
      "Epoch 44/1000\n",
      "27196/27196 [==============================] - 4s 150us/step - loss: 5.0029 - accuracy: 0.2158 - val_loss: 6.1405 - val_accuracy: 0.1941\n",
      "Epoch 45/1000\n",
      "27196/27196 [==============================] - 4s 148us/step - loss: 4.9702 - accuracy: 0.2198 - val_loss: 6.1426 - val_accuracy: 0.1931\n",
      "Epoch 46/1000\n",
      "27196/27196 [==============================] - 4s 148us/step - loss: 4.9548 - accuracy: 0.2200 - val_loss: 6.1432 - val_accuracy: 0.1966\n",
      "Epoch 47/1000\n",
      "27196/27196 [==============================] - 4s 149us/step - loss: 4.9257 - accuracy: 0.2229 - val_loss: 6.1496 - val_accuracy: 0.1958\n",
      "Epoch 48/1000\n",
      "27196/27196 [==============================] - 4s 148us/step - loss: 4.9028 - accuracy: 0.2251 - val_loss: 6.1474 - val_accuracy: 0.1967\n",
      "Epoch 49/1000\n",
      "27196/27196 [==============================] - 4s 148us/step - loss: 4.8801 - accuracy: 0.2254 - val_loss: 6.1655 - val_accuracy: 0.1988\n",
      "Epoch 50/1000\n",
      "27196/27196 [==============================] - 4s 148us/step - loss: 4.8636 - accuracy: 0.2283 - val_loss: 6.1586 - val_accuracy: 0.2000\n",
      "Epoch 51/1000\n",
      "27196/27196 [==============================] - 4s 149us/step - loss: 4.8453 - accuracy: 0.2303 - val_loss: 6.1811 - val_accuracy: 0.1988\n",
      "Epoch 52/1000\n",
      "27196/27196 [==============================] - 4s 149us/step - loss: 4.8194 - accuracy: 0.2297 - val_loss: 6.1995 - val_accuracy: 0.1996\n",
      "generating training data for shakespeare\n",
      "dictionary size: \n",
      "19113\n",
      "trainingseqLength: \n",
      "22366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15656, 15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(15656, 19113)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object: y_valid\n",
      "\tSize (GB):  0.128248342\n",
      "Object: y_train\n",
      "\tSize (GB):  0.29923324\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 100)         1911300   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 19113)             2465577   \n",
      "=================================================================\n",
      "Total params: 4,427,437\n",
      "Trainable params: 4,427,437\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15656 samples, validate on 6710 samples\n",
      "Epoch 1/1000\n",
      "15656/15656 [==============================] - 3s 212us/step - loss: 9.8418 - accuracy: 0.0125 - val_loss: 9.7943 - val_accuracy: 0.0687\n",
      "Epoch 2/1000\n",
      "15656/15656 [==============================] - 2s 158us/step - loss: 9.6478 - accuracy: 0.0457 - val_loss: 9.2441 - val_accuracy: 0.0730\n",
      "Epoch 3/1000\n",
      "15656/15656 [==============================] - 2s 157us/step - loss: 8.4366 - accuracy: 0.0560 - val_loss: 7.2048 - val_accuracy: 0.0730\n",
      "Epoch 4/1000\n",
      "15656/15656 [==============================] - 2s 154us/step - loss: 6.4741 - accuracy: 0.0654 - val_loss: 6.3607 - val_accuracy: 0.0730\n",
      "Epoch 5/1000\n",
      "15656/15656 [==============================] - 2s 155us/step - loss: 6.1286 - accuracy: 0.0854 - val_loss: 6.5094 - val_accuracy: 0.0981\n",
      "Epoch 6/1000\n",
      "15656/15656 [==============================] - 2s 154us/step - loss: 6.0160 - accuracy: 0.0941 - val_loss: 6.3502 - val_accuracy: 0.0981\n",
      "Epoch 7/1000\n",
      "15656/15656 [==============================] - 2s 154us/step - loss: 5.9413 - accuracy: 0.0945 - val_loss: 6.3356 - val_accuracy: 0.0981\n",
      "Epoch 8/1000\n",
      "15656/15656 [==============================] - 2s 153us/step - loss: 5.9050 - accuracy: 0.0958 - val_loss: 6.3204 - val_accuracy: 0.0981\n",
      "Epoch 9/1000\n",
      "15656/15656 [==============================] - 2s 157us/step - loss: 5.8907 - accuracy: 0.0946 - val_loss: 6.3231 - val_accuracy: 0.0981\n",
      "Epoch 10/1000\n",
      "15656/15656 [==============================] - 2s 154us/step - loss: 5.8771 - accuracy: 0.0941 - val_loss: 6.3145 - val_accuracy: 0.0981\n",
      "Epoch 11/1000\n",
      "15656/15656 [==============================] - 2s 155us/step - loss: 5.8659 - accuracy: 0.0957 - val_loss: 6.3191 - val_accuracy: 0.0981\n",
      "Epoch 12/1000\n",
      "15656/15656 [==============================] - 2s 154us/step - loss: 5.8637 - accuracy: 0.0944 - val_loss: 6.3249 - val_accuracy: 0.0981\n",
      "Epoch 13/1000\n",
      "15656/15656 [==============================] - 2s 154us/step - loss: 5.8497 - accuracy: 0.0956 - val_loss: 6.3230 - val_accuracy: 0.0981\n",
      "Epoch 14/1000\n",
      "15656/15656 [==============================] - 2s 153us/step - loss: 5.8360 - accuracy: 0.0946 - val_loss: 6.3182 - val_accuracy: 0.0981\n",
      "Epoch 15/1000\n",
      "15656/15656 [==============================] - 2s 154us/step - loss: 5.8170 - accuracy: 0.0946 - val_loss: 6.3103 - val_accuracy: 0.0981\n",
      "Epoch 16/1000\n",
      "15656/15656 [==============================] - 2s 153us/step - loss: 5.8009 - accuracy: 0.0953 - val_loss: 6.2975 - val_accuracy: 0.0981\n",
      "Epoch 17/1000\n",
      "15656/15656 [==============================] - 2s 156us/step - loss: 5.7835 - accuracy: 0.0954 - val_loss: 6.2795 - val_accuracy: 0.0981\n",
      "Epoch 18/1000\n",
      "15656/15656 [==============================] - 2s 155us/step - loss: 5.7600 - accuracy: 0.0955 - val_loss: 6.2601 - val_accuracy: 0.0981\n",
      "Epoch 19/1000\n",
      "15656/15656 [==============================] - 2s 154us/step - loss: 5.7361 - accuracy: 0.0957 - val_loss: 6.2397 - val_accuracy: 0.0981\n",
      "Epoch 20/1000\n",
      "15656/15656 [==============================] - 2s 156us/step - loss: 5.7076 - accuracy: 0.0950 - val_loss: 6.2274 - val_accuracy: 0.0981\n",
      "Epoch 21/1000\n",
      "15656/15656 [==============================] - 2s 155us/step - loss: 5.6854 - accuracy: 0.0960 - val_loss: 6.2203 - val_accuracy: 0.0981\n",
      "Epoch 22/1000\n",
      "15656/15656 [==============================] - 2s 156us/step - loss: 5.6745 - accuracy: 0.0957 - val_loss: 6.2192 - val_accuracy: 0.0981\n",
      "Epoch 23/1000\n",
      "15656/15656 [==============================] - 2s 154us/step - loss: 5.6559 - accuracy: 0.0958 - val_loss: 6.2268 - val_accuracy: 0.0981\n",
      "Epoch 24/1000\n",
      "15656/15656 [==============================] - 2s 154us/step - loss: 5.6454 - accuracy: 0.0953 - val_loss: 6.2369 - val_accuracy: 0.0981\n",
      "Epoch 25/1000\n",
      "15656/15656 [==============================] - 2s 155us/step - loss: 5.6334 - accuracy: 0.0957 - val_loss: 6.2482 - val_accuracy: 0.0981\n",
      "Epoch 26/1000\n",
      "15656/15656 [==============================] - 2s 155us/step - loss: 5.6271 - accuracy: 0.0957 - val_loss: 6.2645 - val_accuracy: 0.0981\n",
      "Epoch 27/1000\n",
      "15656/15656 [==============================] - 2s 155us/step - loss: 5.6231 - accuracy: 0.0959 - val_loss: 6.2767 - val_accuracy: 0.0981\n",
      "Epoch 28/1000\n",
      "15656/15656 [==============================] - 2s 154us/step - loss: 5.6130 - accuracy: 0.0956 - val_loss: 6.2895 - val_accuracy: 0.0981\n",
      "Epoch 29/1000\n",
      "15656/15656 [==============================] - 2s 156us/step - loss: 5.6065 - accuracy: 0.0957 - val_loss: 6.3059 - val_accuracy: 0.0981\n",
      "Epoch 30/1000\n",
      "15656/15656 [==============================] - 2s 156us/step - loss: 5.5984 - accuracy: 0.0957 - val_loss: 6.3185 - val_accuracy: 0.0981\n",
      "Epoch 31/1000\n",
      "15656/15656 [==============================] - 2s 156us/step - loss: 5.5922 - accuracy: 0.0957 - val_loss: 6.3249 - val_accuracy: 0.0981\n",
      "Epoch 32/1000\n",
      "15656/15656 [==============================] - 2s 153us/step - loss: 5.5881 - accuracy: 0.0960 - val_loss: 6.3415 - val_accuracy: 0.0981\n",
      "Epoch 33/1000\n",
      "15656/15656 [==============================] - 2s 155us/step - loss: 5.5864 - accuracy: 0.0956 - val_loss: 6.3506 - val_accuracy: 0.0981\n",
      "Epoch 34/1000\n",
      "15656/15656 [==============================] - 2s 154us/step - loss: 5.5805 - accuracy: 0.0963 - val_loss: 6.3619 - val_accuracy: 0.0981\n",
      "Epoch 35/1000\n",
      "15656/15656 [==============================] - 2s 154us/step - loss: 5.5727 - accuracy: 0.0975 - val_loss: 6.3705 - val_accuracy: 0.0981\n",
      "Epoch 36/1000\n",
      "15656/15656 [==============================] - 2s 155us/step - loss: 5.5738 - accuracy: 0.0968 - val_loss: 6.3804 - val_accuracy: 0.0997\n",
      "Epoch 37/1000\n",
      "15656/15656 [==============================] - 2s 156us/step - loss: 5.5666 - accuracy: 0.0994 - val_loss: 6.3830 - val_accuracy: 0.0997\n",
      "Epoch 38/1000\n",
      "15656/15656 [==============================] - 2s 154us/step - loss: 5.5595 - accuracy: 0.0977 - val_loss: 6.3939 - val_accuracy: 0.1031\n",
      "Epoch 39/1000\n",
      "15656/15656 [==============================] - 2s 155us/step - loss: 5.5519 - accuracy: 0.1015 - val_loss: 6.4011 - val_accuracy: 0.1043\n",
      "Epoch 40/1000\n",
      "15656/15656 [==============================] - 2s 155us/step - loss: 5.5476 - accuracy: 0.1008 - val_loss: 6.4090 - val_accuracy: 0.1061\n",
      "Epoch 41/1000\n",
      "15656/15656 [==============================] - 2s 154us/step - loss: 5.5399 - accuracy: 0.1019 - val_loss: 6.4131 - val_accuracy: 0.1128\n",
      "Epoch 42/1000\n",
      "15656/15656 [==============================] - 2s 156us/step - loss: 5.5303 - accuracy: 0.1062 - val_loss: 6.4128 - val_accuracy: 0.1103\n",
      "Epoch 43/1000\n",
      "15656/15656 [==============================] - 2s 155us/step - loss: 5.5259 - accuracy: 0.1046 - val_loss: 6.4202 - val_accuracy: 0.1177\n",
      "Epoch 44/1000\n",
      "15656/15656 [==============================] - 2s 154us/step - loss: 5.5208 - accuracy: 0.1075 - val_loss: 6.4258 - val_accuracy: 0.1174\n",
      "Epoch 45/1000\n",
      "15656/15656 [==============================] - 2s 156us/step - loss: 5.5115 - accuracy: 0.1092 - val_loss: 6.4345 - val_accuracy: 0.1206\n",
      "Epoch 46/1000\n",
      "15656/15656 [==============================] - 2s 156us/step - loss: 5.5066 - accuracy: 0.1092 - val_loss: 6.4356 - val_accuracy: 0.1212\n",
      "Epoch 47/1000\n",
      "15656/15656 [==============================] - 2s 155us/step - loss: 5.4957 - accuracy: 0.1105 - val_loss: 6.4421 - val_accuracy: 0.1227\n",
      "Epoch 48/1000\n",
      "15656/15656 [==============================] - 2s 155us/step - loss: 5.4880 - accuracy: 0.1102 - val_loss: 6.4495 - val_accuracy: 0.1206\n",
      "Epoch 49/1000\n",
      "15656/15656 [==============================] - 2s 154us/step - loss: 5.4766 - accuracy: 0.1134 - val_loss: 6.4580 - val_accuracy: 0.1256\n",
      "Epoch 50/1000\n",
      "15656/15656 [==============================] - 2s 155us/step - loss: 5.4717 - accuracy: 0.1141 - val_loss: 6.4668 - val_accuracy: 0.1219\n",
      "Epoch 51/1000\n",
      "15656/15656 [==============================] - 2s 153us/step - loss: 5.4631 - accuracy: 0.1147 - val_loss: 6.4760 - val_accuracy: 0.1230\n",
      "Epoch 52/1000\n",
      "15656/15656 [==============================] - 2s 156us/step - loss: 5.4550 - accuracy: 0.1129 - val_loss: 6.4797 - val_accuracy: 0.1241\n",
      "Epoch 53/1000\n",
      "15656/15656 [==============================] - 2s 155us/step - loss: 5.4476 - accuracy: 0.1175 - val_loss: 6.4901 - val_accuracy: 0.1255\n",
      "Epoch 54/1000\n",
      "15656/15656 [==============================] - 2s 157us/step - loss: 5.4401 - accuracy: 0.1134 - val_loss: 6.4965 - val_accuracy: 0.1247\n",
      "Epoch 55/1000\n",
      "15656/15656 [==============================] - 2s 154us/step - loss: 5.4282 - accuracy: 0.1165 - val_loss: 6.5033 - val_accuracy: 0.1264\n",
      "Epoch 56/1000\n",
      "15656/15656 [==============================] - 2s 154us/step - loss: 5.4171 - accuracy: 0.1191 - val_loss: 6.5114 - val_accuracy: 0.1252\n",
      "Epoch 57/1000\n",
      "15656/15656 [==============================] - 2s 153us/step - loss: 5.4061 - accuracy: 0.1191 - val_loss: 6.5196 - val_accuracy: 0.1270\n",
      "Epoch 58/1000\n",
      "15656/15656 [==============================] - 2s 153us/step - loss: 5.3994 - accuracy: 0.1201 - val_loss: 6.5224 - val_accuracy: 0.1304\n",
      "Epoch 59/1000\n",
      "15656/15656 [==============================] - 2s 156us/step - loss: 5.3860 - accuracy: 0.1214 - val_loss: 6.5285 - val_accuracy: 0.1250\n",
      "Epoch 60/1000\n",
      "15656/15656 [==============================] - 2s 153us/step - loss: 5.3695 - accuracy: 0.1210 - val_loss: 6.5399 - val_accuracy: 0.1292\n",
      "Epoch 61/1000\n",
      "15656/15656 [==============================] - 2s 153us/step - loss: 5.3568 - accuracy: 0.1244 - val_loss: 6.5440 - val_accuracy: 0.1277\n",
      "Epoch 62/1000\n",
      "15656/15656 [==============================] - 2s 153us/step - loss: 5.3415 - accuracy: 0.1256 - val_loss: 6.5482 - val_accuracy: 0.1326\n",
      "generating training data for seuss\n",
      "dictionary size: \n",
      "19113\n",
      "trainingseqLength: \n",
      "21051\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14735, 15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(14735, 19113)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object: y_valid\n",
      "\tSize (GB):  0.12071782\n",
      "Object: y_train\n",
      "\tSize (GB):  0.281630167\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 100)         1911300   \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 19113)             2465577   \n",
      "=================================================================\n",
      "Total params: 4,427,437\n",
      "Trainable params: 4,427,437\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14735 samples, validate on 6316 samples\n",
      "Epoch 1/1000\n",
      "14735/14735 [==============================] - 3s 212us/step - loss: 9.8401 - accuracy: 0.0099 - val_loss: 9.7762 - val_accuracy: 0.0975\n",
      "Epoch 2/1000\n",
      "14735/14735 [==============================] - 2s 148us/step - loss: 9.6110 - accuracy: 0.0419 - val_loss: 9.0983 - val_accuracy: 0.0963\n",
      "Epoch 3/1000\n",
      "14735/14735 [==============================] - 2s 147us/step - loss: 8.3678 - accuracy: 0.0706 - val_loss: 7.1214 - val_accuracy: 0.0963\n",
      "Epoch 4/1000\n",
      "14735/14735 [==============================] - 2s 145us/step - loss: 6.3674 - accuracy: 0.0832 - val_loss: 5.9811 - val_accuracy: 0.0963\n",
      "Epoch 5/1000\n",
      "14735/14735 [==============================] - 2s 144us/step - loss: 5.8377 - accuracy: 0.0895 - val_loss: 6.1729 - val_accuracy: 0.0963\n",
      "Epoch 6/1000\n",
      "14735/14735 [==============================] - 2s 144us/step - loss: 5.7953 - accuracy: 0.0922 - val_loss: 6.0007 - val_accuracy: 0.0963\n",
      "Epoch 7/1000\n",
      "14735/14735 [==============================] - 2s 146us/step - loss: 5.6879 - accuracy: 0.0917 - val_loss: 5.9982 - val_accuracy: 0.0963\n",
      "Epoch 8/1000\n",
      "14735/14735 [==============================] - 2s 147us/step - loss: 5.6655 - accuracy: 0.0930 - val_loss: 5.9545 - val_accuracy: 0.0963\n",
      "Epoch 9/1000\n",
      "14735/14735 [==============================] - 2s 146us/step - loss: 5.6393 - accuracy: 0.0939 - val_loss: 5.9507 - val_accuracy: 0.0963\n",
      "Epoch 10/1000\n",
      "14735/14735 [==============================] - 2s 146us/step - loss: 5.6311 - accuracy: 0.0934 - val_loss: 5.9411 - val_accuracy: 0.0963\n",
      "Epoch 11/1000\n",
      "14735/14735 [==============================] - 2s 145us/step - loss: 5.6244 - accuracy: 0.0922 - val_loss: 5.9381 - val_accuracy: 0.0963\n",
      "Epoch 12/1000\n",
      "14735/14735 [==============================] - 2s 145us/step - loss: 5.6171 - accuracy: 0.0938 - val_loss: 5.9381 - val_accuracy: 0.0963\n",
      "Epoch 13/1000\n",
      "14735/14735 [==============================] - 2s 143us/step - loss: 5.6056 - accuracy: 0.0941 - val_loss: 5.9387 - val_accuracy: 0.0963\n",
      "Epoch 14/1000\n",
      "14735/14735 [==============================] - 2s 146us/step - loss: 5.6090 - accuracy: 0.0937 - val_loss: 5.9347 - val_accuracy: 0.0963\n",
      "Epoch 15/1000\n",
      "14735/14735 [==============================] - 2s 146us/step - loss: 5.5935 - accuracy: 0.0939 - val_loss: 5.9267 - val_accuracy: 0.0963\n",
      "Epoch 16/1000\n",
      "14735/14735 [==============================] - 2s 145us/step - loss: 5.5819 - accuracy: 0.0941 - val_loss: 5.9152 - val_accuracy: 0.0963\n",
      "Epoch 17/1000\n",
      "14735/14735 [==============================] - 2s 144us/step - loss: 5.5700 - accuracy: 0.0940 - val_loss: 5.9013 - val_accuracy: 0.0963\n",
      "Epoch 18/1000\n",
      "14735/14735 [==============================] - 2s 145us/step - loss: 5.5500 - accuracy: 0.0940 - val_loss: 5.8842 - val_accuracy: 0.0963\n",
      "Epoch 19/1000\n",
      "14735/14735 [==============================] - 2s 145us/step - loss: 5.5348 - accuracy: 0.0939 - val_loss: 5.8624 - val_accuracy: 0.0963\n",
      "Epoch 20/1000\n",
      "14735/14735 [==============================] - 2s 145us/step - loss: 5.5139 - accuracy: 0.0941 - val_loss: 5.8388 - val_accuracy: 0.0963\n",
      "Epoch 21/1000\n",
      "14735/14735 [==============================] - 2s 145us/step - loss: 5.4884 - accuracy: 0.0940 - val_loss: 5.8207 - val_accuracy: 0.0963\n",
      "Epoch 22/1000\n",
      "14735/14735 [==============================] - 2s 145us/step - loss: 5.4680 - accuracy: 0.0941 - val_loss: 5.8042 - val_accuracy: 0.0963\n",
      "Epoch 23/1000\n",
      "14735/14735 [==============================] - 2s 146us/step - loss: 5.4545 - accuracy: 0.0940 - val_loss: 5.7968 - val_accuracy: 0.0963\n",
      "Epoch 24/1000\n",
      "14735/14735 [==============================] - 2s 143us/step - loss: 5.4379 - accuracy: 0.0940 - val_loss: 5.7946 - val_accuracy: 0.0963\n",
      "Epoch 25/1000\n",
      "14735/14735 [==============================] - 2s 146us/step - loss: 5.4266 - accuracy: 0.0938 - val_loss: 5.7935 - val_accuracy: 0.0963\n",
      "Epoch 26/1000\n",
      "14735/14735 [==============================] - 2s 145us/step - loss: 5.4187 - accuracy: 0.0941 - val_loss: 5.7983 - val_accuracy: 0.0963\n",
      "Epoch 27/1000\n",
      "14735/14735 [==============================] - 2s 145us/step - loss: 5.4080 - accuracy: 0.0939 - val_loss: 5.8084 - val_accuracy: 0.0963\n",
      "Epoch 28/1000\n",
      "14735/14735 [==============================] - 2s 146us/step - loss: 5.4026 - accuracy: 0.0940 - val_loss: 5.8134 - val_accuracy: 0.0963\n",
      "Epoch 29/1000\n",
      "14735/14735 [==============================] - 2s 146us/step - loss: 5.3934 - accuracy: 0.0941 - val_loss: 5.8177 - val_accuracy: 0.0963\n",
      "Epoch 30/1000\n",
      "14735/14735 [==============================] - 2s 145us/step - loss: 5.3814 - accuracy: 0.0941 - val_loss: 5.8242 - val_accuracy: 0.0963\n",
      "Epoch 31/1000\n",
      "14735/14735 [==============================] - 2s 145us/step - loss: 5.3757 - accuracy: 0.0941 - val_loss: 5.8292 - val_accuracy: 0.0963\n",
      "Epoch 32/1000\n",
      "14735/14735 [==============================] - 2s 145us/step - loss: 5.3676 - accuracy: 0.0939 - val_loss: 5.8348 - val_accuracy: 0.0963\n",
      "Epoch 33/1000\n",
      "14735/14735 [==============================] - 2s 144us/step - loss: 5.3634 - accuracy: 0.0941 - val_loss: 5.8393 - val_accuracy: 0.0959\n",
      "Epoch 34/1000\n",
      "14735/14735 [==============================] - 2s 145us/step - loss: 5.3539 - accuracy: 0.0956 - val_loss: 5.8421 - val_accuracy: 0.0953\n",
      "Epoch 35/1000\n",
      "14735/14735 [==============================] - 2s 147us/step - loss: 5.3442 - accuracy: 0.0968 - val_loss: 5.8488 - val_accuracy: 0.0958\n",
      "Epoch 36/1000\n",
      "14735/14735 [==============================] - 2s 146us/step - loss: 5.3349 - accuracy: 0.0973 - val_loss: 5.8472 - val_accuracy: 0.0963\n",
      "Epoch 37/1000\n",
      "14735/14735 [==============================] - 2s 146us/step - loss: 5.3326 - accuracy: 0.0989 - val_loss: 5.8533 - val_accuracy: 0.0967\n",
      "Epoch 38/1000\n",
      "14735/14735 [==============================] - 2s 146us/step - loss: 5.3256 - accuracy: 0.0992 - val_loss: 5.8610 - val_accuracy: 0.0967\n",
      "Epoch 39/1000\n",
      "14735/14735 [==============================] - 2s 145us/step - loss: 5.3195 - accuracy: 0.0990 - val_loss: 5.8612 - val_accuracy: 0.1043\n",
      "Epoch 40/1000\n",
      "14735/14735 [==============================] - 2s 143us/step - loss: 5.3164 - accuracy: 0.0995 - val_loss: 5.8646 - val_accuracy: 0.1053\n",
      "Epoch 41/1000\n",
      "14735/14735 [==============================] - 2s 146us/step - loss: 5.3033 - accuracy: 0.1002 - val_loss: 5.8666 - val_accuracy: 0.1069\n",
      "Epoch 42/1000\n",
      "14735/14735 [==============================] - 2s 147us/step - loss: 5.3007 - accuracy: 0.0994 - val_loss: 5.8734 - val_accuracy: 0.1064\n",
      "Epoch 43/1000\n",
      "14735/14735 [==============================] - 2s 146us/step - loss: 5.2975 - accuracy: 0.1000 - val_loss: 5.8756 - val_accuracy: 0.1080\n",
      "Epoch 44/1000\n",
      "14735/14735 [==============================] - 2s 146us/step - loss: 5.2896 - accuracy: 0.1000 - val_loss: 5.8793 - val_accuracy: 0.1097\n",
      "Epoch 45/1000\n",
      "14735/14735 [==============================] - 2s 145us/step - loss: 5.2902 - accuracy: 0.1004 - val_loss: 5.8821 - val_accuracy: 0.1072\n",
      "Epoch 46/1000\n",
      "14735/14735 [==============================] - 2s 145us/step - loss: 5.2844 - accuracy: 0.1003 - val_loss: 5.8774 - val_accuracy: 0.1094\n",
      "Epoch 47/1000\n",
      "14735/14735 [==============================] - 2s 145us/step - loss: 5.2766 - accuracy: 0.1017 - val_loss: 5.8842 - val_accuracy: 0.1127\n",
      "Epoch 48/1000\n",
      "14735/14735 [==============================] - 2s 144us/step - loss: 5.2673 - accuracy: 0.0994 - val_loss: 5.8907 - val_accuracy: 0.1086\n",
      "Epoch 49/1000\n",
      "14735/14735 [==============================] - 2s 146us/step - loss: 5.2662 - accuracy: 0.1011 - val_loss: 5.8897 - val_accuracy: 0.1132\n",
      "Epoch 50/1000\n",
      "14735/14735 [==============================] - 2s 147us/step - loss: 5.2566 - accuracy: 0.1023 - val_loss: 5.8948 - val_accuracy: 0.1129\n",
      "Epoch 51/1000\n",
      "14735/14735 [==============================] - 2s 146us/step - loss: 5.2487 - accuracy: 0.1032 - val_loss: 5.8987 - val_accuracy: 0.1138\n",
      "Epoch 52/1000\n",
      "14735/14735 [==============================] - 2s 145us/step - loss: 5.2401 - accuracy: 0.1030 - val_loss: 5.9018 - val_accuracy: 0.1138\n",
      "Epoch 53/1000\n",
      "14735/14735 [==============================] - 2s 146us/step - loss: 5.2361 - accuracy: 0.1035 - val_loss: 5.9021 - val_accuracy: 0.1132\n",
      "Epoch 54/1000\n",
      "14735/14735 [==============================] - 2s 147us/step - loss: 5.2234 - accuracy: 0.1045 - val_loss: 5.8965 - val_accuracy: 0.1129\n",
      "Epoch 55/1000\n",
      "14735/14735 [==============================] - 2s 146us/step - loss: 5.2183 - accuracy: 0.1051 - val_loss: 5.9029 - val_accuracy: 0.1129\n",
      "Epoch 56/1000\n",
      "14735/14735 [==============================] - 2s 147us/step - loss: 5.2098 - accuracy: 0.1073 - val_loss: 5.9036 - val_accuracy: 0.1142\n",
      "Epoch 57/1000\n",
      "14735/14735 [==============================] - 2s 145us/step - loss: 5.1984 - accuracy: 0.1091 - val_loss: 5.9083 - val_accuracy: 0.1140\n",
      "Epoch 58/1000\n",
      "14735/14735 [==============================] - 2s 146us/step - loss: 5.1878 - accuracy: 0.1093 - val_loss: 5.9060 - val_accuracy: 0.1138\n",
      "Epoch 59/1000\n",
      "14735/14735 [==============================] - 2s 146us/step - loss: 5.1791 - accuracy: 0.1116 - val_loss: 5.9089 - val_accuracy: 0.1142\n",
      "Epoch 60/1000\n",
      "14735/14735 [==============================] - 2s 145us/step - loss: 5.1626 - accuracy: 0.1092 - val_loss: 5.9134 - val_accuracy: 0.1140\n",
      "Epoch 61/1000\n",
      "14735/14735 [==============================] - 2s 145us/step - loss: 5.1643 - accuracy: 0.1106 - val_loss: 5.9100 - val_accuracy: 0.1138\n",
      "Epoch 62/1000\n",
      "14735/14735 [==============================] - 2s 146us/step - loss: 5.1547 - accuracy: 0.1122 - val_loss: 5.9138 - val_accuracy: 0.1140\n",
      "Epoch 63/1000\n",
      "14735/14735 [==============================] - 2s 145us/step - loss: 5.1418 - accuracy: 0.1107 - val_loss: 5.9083 - val_accuracy: 0.1138\n",
      "Epoch 64/1000\n",
      "14735/14735 [==============================] - 2s 147us/step - loss: 5.1313 - accuracy: 0.1113 - val_loss: 5.9045 - val_accuracy: 0.1142\n",
      "Epoch 65/1000\n",
      "14735/14735 [==============================] - 2s 146us/step - loss: 5.1172 - accuracy: 0.1128 - val_loss: 5.9012 - val_accuracy: 0.1218\n",
      "generating training data for frost\n",
      "dictionary size: \n",
      "19113\n",
      "trainingseqLength: \n",
      "28912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20238, 15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(20238, 19113)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object: y_valid\n",
      "\tSize (GB):  0.165786274\n",
      "Object: y_train\n",
      "\tSize (GB):  0.386809006\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, None, 100)         1911300   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 19113)             2465577   \n",
      "=================================================================\n",
      "Total params: 4,427,437\n",
      "Trainable params: 4,427,437\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20238 samples, validate on 8674 samples\n",
      "Epoch 1/1000\n",
      "20238/20238 [==============================] - 4s 196us/step - loss: 9.8239 - accuracy: 0.0394 - val_loss: 9.7157 - val_accuracy: 0.0963\n",
      "Epoch 2/1000\n",
      "20238/20238 [==============================] - 3s 157us/step - loss: 9.2133 - accuracy: 0.0551 - val_loss: 8.0524 - val_accuracy: 0.0458\n",
      "Epoch 3/1000\n",
      "20238/20238 [==============================] - 3s 147us/step - loss: 6.8530 - accuracy: 0.0541 - val_loss: 6.1272 - val_accuracy: 0.0458\n",
      "Epoch 4/1000\n",
      "20238/20238 [==============================] - 3s 147us/step - loss: 5.9945 - accuracy: 0.0814 - val_loss: 6.2694 - val_accuracy: 0.0966\n",
      "Epoch 5/1000\n",
      "20238/20238 [==============================] - 3s 146us/step - loss: 5.8724 - accuracy: 0.0997 - val_loss: 6.1188 - val_accuracy: 0.0966\n",
      "Epoch 6/1000\n",
      "20238/20238 [==============================] - 3s 145us/step - loss: 5.8089 - accuracy: 0.0995 - val_loss: 6.0797 - val_accuracy: 0.0966\n",
      "Epoch 7/1000\n",
      "20238/20238 [==============================] - 3s 145us/step - loss: 5.7702 - accuracy: 0.0998 - val_loss: 6.0815 - val_accuracy: 0.0966\n",
      "Epoch 8/1000\n",
      "20238/20238 [==============================] - 3s 145us/step - loss: 5.7531 - accuracy: 0.0998 - val_loss: 6.0700 - val_accuracy: 0.0966\n",
      "Epoch 9/1000\n",
      "20238/20238 [==============================] - 3s 146us/step - loss: 5.7490 - accuracy: 0.0999 - val_loss: 6.0737 - val_accuracy: 0.0966\n",
      "Epoch 10/1000\n",
      "20238/20238 [==============================] - 3s 145us/step - loss: 5.7329 - accuracy: 0.0999 - val_loss: 6.0749 - val_accuracy: 0.0966\n",
      "Epoch 11/1000\n",
      "20238/20238 [==============================] - 3s 147us/step - loss: 5.7216 - accuracy: 0.0999 - val_loss: 6.0704 - val_accuracy: 0.0966\n",
      "Epoch 12/1000\n",
      "20238/20238 [==============================] - 3s 147us/step - loss: 5.7068 - accuracy: 0.0999 - val_loss: 6.0638 - val_accuracy: 0.0966\n",
      "Epoch 13/1000\n",
      "20238/20238 [==============================] - 3s 147us/step - loss: 5.6900 - accuracy: 0.0999 - val_loss: 6.0497 - val_accuracy: 0.0966\n",
      "Epoch 14/1000\n",
      "20238/20238 [==============================] - 3s 145us/step - loss: 5.6692 - accuracy: 0.0999 - val_loss: 6.0276 - val_accuracy: 0.0966\n",
      "Epoch 15/1000\n",
      "20238/20238 [==============================] - 3s 146us/step - loss: 5.6427 - accuracy: 0.0999 - val_loss: 6.0021 - val_accuracy: 0.0966\n",
      "Epoch 16/1000\n",
      "20238/20238 [==============================] - 3s 144us/step - loss: 5.6157 - accuracy: 0.0999 - val_loss: 5.9771 - val_accuracy: 0.0966\n",
      "Epoch 17/1000\n",
      "20238/20238 [==============================] - 3s 145us/step - loss: 5.5825 - accuracy: 0.0999 - val_loss: 5.9556 - val_accuracy: 0.0966\n",
      "Epoch 18/1000\n",
      "20238/20238 [==============================] - 3s 146us/step - loss: 5.5577 - accuracy: 0.0999 - val_loss: 5.9455 - val_accuracy: 0.0966\n",
      "Epoch 19/1000\n",
      "20238/20238 [==============================] - 3s 146us/step - loss: 5.5409 - accuracy: 0.0999 - val_loss: 5.9467 - val_accuracy: 0.0966\n",
      "Epoch 20/1000\n",
      "20238/20238 [==============================] - 3s 145us/step - loss: 5.5285 - accuracy: 0.0999 - val_loss: 5.9555 - val_accuracy: 0.0966\n",
      "Epoch 21/1000\n",
      "20238/20238 [==============================] - 3s 144us/step - loss: 5.5170 - accuracy: 0.0999 - val_loss: 5.9644 - val_accuracy: 0.0966\n",
      "Epoch 22/1000\n",
      "20238/20238 [==============================] - 3s 147us/step - loss: 5.5058 - accuracy: 0.0999 - val_loss: 5.9772 - val_accuracy: 0.0966\n",
      "Epoch 23/1000\n",
      "20238/20238 [==============================] - 3s 147us/step - loss: 5.4982 - accuracy: 0.0999 - val_loss: 5.9865 - val_accuracy: 0.0966\n",
      "Epoch 24/1000\n",
      "20238/20238 [==============================] - 3s 146us/step - loss: 5.4898 - accuracy: 0.0999 - val_loss: 6.0003 - val_accuracy: 0.0966\n",
      "Epoch 25/1000\n",
      "20238/20238 [==============================] - 3s 147us/step - loss: 5.4819 - accuracy: 0.0999 - val_loss: 6.0074 - val_accuracy: 0.0966\n",
      "Epoch 26/1000\n",
      "20238/20238 [==============================] - 3s 145us/step - loss: 5.4774 - accuracy: 0.0998 - val_loss: 6.0170 - val_accuracy: 0.0966\n",
      "Epoch 27/1000\n",
      "20238/20238 [==============================] - 3s 145us/step - loss: 5.4680 - accuracy: 0.0999 - val_loss: 6.0238 - val_accuracy: 0.0964\n",
      "Epoch 28/1000\n",
      "20238/20238 [==============================] - 3s 145us/step - loss: 5.4551 - accuracy: 0.1002 - val_loss: 6.0358 - val_accuracy: 0.0963\n",
      "Epoch 29/1000\n",
      "20238/20238 [==============================] - 3s 147us/step - loss: 5.4473 - accuracy: 0.1018 - val_loss: 6.0401 - val_accuracy: 0.0976\n",
      "Epoch 30/1000\n",
      "20238/20238 [==============================] - 3s 145us/step - loss: 5.4399 - accuracy: 0.1039 - val_loss: 6.0463 - val_accuracy: 0.0982\n",
      "Epoch 31/1000\n",
      "20238/20238 [==============================] - 3s 147us/step - loss: 5.4337 - accuracy: 0.1038 - val_loss: 6.0549 - val_accuracy: 0.0973\n",
      "Epoch 32/1000\n",
      "20238/20238 [==============================] - 3s 146us/step - loss: 5.4242 - accuracy: 0.1047 - val_loss: 6.0631 - val_accuracy: 0.0997\n",
      "Epoch 33/1000\n",
      "20238/20238 [==============================] - 3s 147us/step - loss: 5.4109 - accuracy: 0.1073 - val_loss: 6.0677 - val_accuracy: 0.1011\n",
      "Epoch 34/1000\n",
      "20238/20238 [==============================] - 3s 146us/step - loss: 5.4066 - accuracy: 0.1067 - val_loss: 6.0753 - val_accuracy: 0.1026\n",
      "Epoch 35/1000\n",
      "20238/20238 [==============================] - 3s 146us/step - loss: 5.3928 - accuracy: 0.1076 - val_loss: 6.0825 - val_accuracy: 0.1032\n",
      "Epoch 36/1000\n",
      "20238/20238 [==============================] - 3s 146us/step - loss: 5.3843 - accuracy: 0.1089 - val_loss: 6.0887 - val_accuracy: 0.1031\n",
      "Epoch 37/1000\n",
      "20238/20238 [==============================] - 3s 146us/step - loss: 5.3800 - accuracy: 0.1066 - val_loss: 6.0948 - val_accuracy: 0.1050\n",
      "Epoch 38/1000\n",
      "20238/20238 [==============================] - 3s 146us/step - loss: 5.3719 - accuracy: 0.1090 - val_loss: 6.1044 - val_accuracy: 0.1048\n",
      "Epoch 39/1000\n",
      "20238/20238 [==============================] - 3s 145us/step - loss: 5.3616 - accuracy: 0.1101 - val_loss: 6.1118 - val_accuracy: 0.1048\n",
      "Epoch 40/1000\n",
      "20238/20238 [==============================] - 3s 144us/step - loss: 5.3556 - accuracy: 0.1103 - val_loss: 6.1187 - val_accuracy: 0.1053\n",
      "Epoch 41/1000\n",
      "20238/20238 [==============================] - 3s 144us/step - loss: 5.3477 - accuracy: 0.1102 - val_loss: 6.1258 - val_accuracy: 0.1061\n",
      "Epoch 42/1000\n",
      "20238/20238 [==============================] - 3s 147us/step - loss: 5.3411 - accuracy: 0.1117 - val_loss: 6.1339 - val_accuracy: 0.1059\n",
      "Epoch 43/1000\n",
      "20238/20238 [==============================] - 3s 147us/step - loss: 5.3350 - accuracy: 0.1123 - val_loss: 6.1436 - val_accuracy: 0.1062\n",
      "Epoch 44/1000\n",
      "20238/20238 [==============================] - 3s 146us/step - loss: 5.3318 - accuracy: 0.1117 - val_loss: 6.1470 - val_accuracy: 0.1065\n",
      "Epoch 45/1000\n",
      "20238/20238 [==============================] - 3s 146us/step - loss: 5.3228 - accuracy: 0.1128 - val_loss: 6.1545 - val_accuracy: 0.1068\n",
      "Epoch 46/1000\n",
      "20238/20238 [==============================] - 3s 145us/step - loss: 5.3119 - accuracy: 0.1133 - val_loss: 6.1593 - val_accuracy: 0.1068\n",
      "Epoch 47/1000\n",
      "20238/20238 [==============================] - 3s 145us/step - loss: 5.3098 - accuracy: 0.1148 - val_loss: 6.1675 - val_accuracy: 0.1073\n",
      "Epoch 48/1000\n",
      "20238/20238 [==============================] - 3s 146us/step - loss: 5.3048 - accuracy: 0.1156 - val_loss: 6.1713 - val_accuracy: 0.1074\n",
      "Epoch 49/1000\n",
      "20238/20238 [==============================] - 3s 145us/step - loss: 5.2958 - accuracy: 0.1151 - val_loss: 6.1769 - val_accuracy: 0.1076\n",
      "Epoch 50/1000\n",
      "20238/20238 [==============================] - 3s 144us/step - loss: 5.2895 - accuracy: 0.1179 - val_loss: 6.1778 - val_accuracy: 0.1083\n",
      "Epoch 51/1000\n",
      "20238/20238 [==============================] - 3s 146us/step - loss: 5.2776 - accuracy: 0.1169 - val_loss: 6.1786 - val_accuracy: 0.1088\n",
      "Epoch 52/1000\n",
      "20238/20238 [==============================] - 3s 146us/step - loss: 5.2719 - accuracy: 0.1174 - val_loss: 6.1754 - val_accuracy: 0.1085\n",
      "Epoch 53/1000\n",
      "20238/20238 [==============================] - 3s 145us/step - loss: 5.2574 - accuracy: 0.1189 - val_loss: 6.1816 - val_accuracy: 0.1102\n",
      "Epoch 54/1000\n",
      "20238/20238 [==============================] - 3s 146us/step - loss: 5.2455 - accuracy: 0.1192 - val_loss: 6.1716 - val_accuracy: 0.1129\n",
      "Epoch 55/1000\n",
      "20238/20238 [==============================] - 3s 145us/step - loss: 5.2334 - accuracy: 0.1218 - val_loss: 6.1694 - val_accuracy: 0.1115\n",
      "Epoch 56/1000\n",
      "20238/20238 [==============================] - 3s 144us/step - loss: 5.2234 - accuracy: 0.1211 - val_loss: 6.1629 - val_accuracy: 0.1129\n",
      "Epoch 57/1000\n",
      "20238/20238 [==============================] - 3s 146us/step - loss: 5.2051 - accuracy: 0.1228 - val_loss: 6.1616 - val_accuracy: 0.1172\n",
      "Epoch 58/1000\n",
      "20238/20238 [==============================] - 3s 146us/step - loss: 5.1898 - accuracy: 0.1243 - val_loss: 6.1509 - val_accuracy: 0.1151\n",
      "generating training data for whitman\n",
      "dictionary size: \n",
      "19113\n",
      "trainingseqLength: \n",
      "167005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(116903, 15)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(116903, 19113)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object: y_valid\n",
      "\tSize (GB):  0.957599638\n",
      "Object: y_train\n",
      "\tSize (GB):  2.234367151\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, None, 100)         1911300   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 19113)             2465577   \n",
      "=================================================================\n",
      "Total params: 4,427,437\n",
      "Trainable params: 4,427,437\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 116903 samples, validate on 50102 samples\n",
      "Epoch 1/1000\n",
      "116903/116903 [==============================] - 20s 169us/step - loss: 7.7321 - accuracy: 0.0751 - val_loss: 6.2663 - val_accuracy: 0.1096\n",
      "Epoch 2/1000\n",
      "116903/116903 [==============================] - 18s 153us/step - loss: 6.1941 - accuracy: 0.1086 - val_loss: 6.1979 - val_accuracy: 0.1096\n",
      "Epoch 3/1000\n",
      "116903/116903 [==============================] - 18s 154us/step - loss: 6.0768 - accuracy: 0.1094 - val_loss: 6.1403 - val_accuracy: 0.1096\n",
      "Epoch 4/1000\n",
      "116903/116903 [==============================] - 18s 156us/step - loss: 6.0094 - accuracy: 0.1101 - val_loss: 6.1564 - val_accuracy: 0.1096\n",
      "Epoch 5/1000\n",
      "116903/116903 [==============================] - 18s 154us/step - loss: 5.9818 - accuracy: 0.1104 - val_loss: 6.1730 - val_accuracy: 0.1096\n",
      "Epoch 6/1000\n",
      "116903/116903 [==============================] - 18s 158us/step - loss: 5.9600 - accuracy: 0.1117 - val_loss: 6.1876 - val_accuracy: 0.1096\n",
      "Epoch 7/1000\n",
      "116903/116903 [==============================] - 18s 158us/step - loss: 5.9353 - accuracy: 0.1173 - val_loss: 6.1918 - val_accuracy: 0.1111\n",
      "Epoch 8/1000\n",
      "116903/116903 [==============================] - 18s 158us/step - loss: 5.8952 - accuracy: 0.1392 - val_loss: 6.1607 - val_accuracy: 0.1646\n",
      "Epoch 9/1000\n",
      "116903/116903 [==============================] - 18s 157us/step - loss: 5.8131 - accuracy: 0.1739 - val_loss: 6.0648 - val_accuracy: 0.1992\n",
      "Epoch 10/1000\n",
      "116903/116903 [==============================] - 19s 160us/step - loss: 5.7154 - accuracy: 0.2005 - val_loss: 6.0199 - val_accuracy: 0.2074\n",
      "Epoch 11/1000\n",
      "116903/116903 [==============================] - 18s 157us/step - loss: 5.6478 - accuracy: 0.2117 - val_loss: 5.9996 - val_accuracy: 0.2105\n",
      "Epoch 12/1000\n",
      "116903/116903 [==============================] - 19s 159us/step - loss: 5.5870 - accuracy: 0.2178 - val_loss: 5.9832 - val_accuracy: 0.2107\n",
      "Epoch 13/1000\n",
      "116903/116903 [==============================] - 19s 159us/step - loss: 5.5335 - accuracy: 0.2196 - val_loss: 5.9899 - val_accuracy: 0.2115\n",
      "Epoch 14/1000\n",
      "116903/116903 [==============================] - 18s 156us/step - loss: 5.4721 - accuracy: 0.2208 - val_loss: 5.9729 - val_accuracy: 0.2137\n",
      "Epoch 15/1000\n",
      "116903/116903 [==============================] - 18s 158us/step - loss: 5.4167 - accuracy: 0.2231 - val_loss: 5.9650 - val_accuracy: 0.2157\n",
      "Epoch 16/1000\n",
      "116903/116903 [==============================] - 19s 159us/step - loss: 5.3635 - accuracy: 0.2252 - val_loss: 5.9502 - val_accuracy: 0.2170\n",
      "Epoch 17/1000\n",
      "116903/116903 [==============================] - 18s 157us/step - loss: 5.3066 - accuracy: 0.2278 - val_loss: 5.9424 - val_accuracy: 0.2188\n",
      "Epoch 18/1000\n",
      "116903/116903 [==============================] - 19s 159us/step - loss: 5.2605 - accuracy: 0.2295 - val_loss: 5.9502 - val_accuracy: 0.2191\n",
      "Epoch 19/1000\n",
      "116903/116903 [==============================] - 19s 160us/step - loss: 5.2197 - accuracy: 0.2310 - val_loss: 5.9677 - val_accuracy: 0.2197\n",
      "Epoch 20/1000\n",
      "116903/116903 [==============================] - 18s 158us/step - loss: 5.1766 - accuracy: 0.2339 - val_loss: 5.9742 - val_accuracy: 0.2206\n",
      "Epoch 21/1000\n",
      "116903/116903 [==============================] - 18s 158us/step - loss: 5.1370 - accuracy: 0.2367 - val_loss: 5.9735 - val_accuracy: 0.2215\n",
      "Epoch 22/1000\n",
      "116903/116903 [==============================] - 19s 160us/step - loss: 5.0962 - accuracy: 0.2391 - val_loss: 5.9935 - val_accuracy: 0.2244\n",
      "Epoch 23/1000\n",
      "116903/116903 [==============================] - 18s 157us/step - loss: 5.0561 - accuracy: 0.2434 - val_loss: 5.9897 - val_accuracy: 0.2250\n",
      "Epoch 24/1000\n",
      "116903/116903 [==============================] - 19s 160us/step - loss: 5.0201 - accuracy: 0.2450 - val_loss: 6.0259 - val_accuracy: 0.2260\n",
      "Epoch 25/1000\n",
      "116903/116903 [==============================] - 19s 159us/step - loss: 4.9820 - accuracy: 0.2479 - val_loss: 6.0179 - val_accuracy: 0.2267\n",
      "Epoch 26/1000\n",
      "116903/116903 [==============================] - 18s 158us/step - loss: 4.9470 - accuracy: 0.2494 - val_loss: 6.0316 - val_accuracy: 0.2266\n",
      "Epoch 27/1000\n",
      "116903/116903 [==============================] - 19s 158us/step - loss: 4.9131 - accuracy: 0.2517 - val_loss: 6.0538 - val_accuracy: 0.2278\n",
      "Epoch 28/1000\n",
      "116903/116903 [==============================] - 19s 161us/step - loss: 4.8792 - accuracy: 0.2538 - val_loss: 6.0855 - val_accuracy: 0.2281\n",
      "Epoch 29/1000\n",
      "116903/116903 [==============================] - 18s 157us/step - loss: 4.8419 - accuracy: 0.2549 - val_loss: 6.1245 - val_accuracy: 0.2286\n",
      "Epoch 30/1000\n",
      "116903/116903 [==============================] - 19s 158us/step - loss: 4.8115 - accuracy: 0.2571 - val_loss: 6.1322 - val_accuracy: 0.2286\n",
      "Epoch 31/1000\n",
      "116903/116903 [==============================] - 18s 158us/step - loss: 4.7803 - accuracy: 0.2590 - val_loss: 6.1708 - val_accuracy: 0.2283\n",
      "Epoch 32/1000\n",
      "116903/116903 [==============================] - 19s 159us/step - loss: 4.7497 - accuracy: 0.2600 - val_loss: 6.1696 - val_accuracy: 0.2267\n",
      "Epoch 33/1000\n",
      "116903/116903 [==============================] - 18s 158us/step - loss: 4.7170 - accuracy: 0.2622 - val_loss: 6.2110 - val_accuracy: 0.2295\n",
      "Epoch 34/1000\n",
      "116903/116903 [==============================] - 19s 160us/step - loss: 4.6832 - accuracy: 0.2631 - val_loss: 6.2451 - val_accuracy: 0.2276\n",
      "Epoch 35/1000\n",
      "116903/116903 [==============================] - 18s 158us/step - loss: 4.6561 - accuracy: 0.2652 - val_loss: 6.2768 - val_accuracy: 0.2270\n",
      "Epoch 36/1000\n",
      "116903/116903 [==============================] - 19s 158us/step - loss: 4.6277 - accuracy: 0.2673 - val_loss: 6.3097 - val_accuracy: 0.2277\n",
      "Epoch 37/1000\n",
      "116903/116903 [==============================] - 19s 160us/step - loss: 4.5973 - accuracy: 0.2685 - val_loss: 6.3199 - val_accuracy: 0.2276\n",
      "Epoch 38/1000\n",
      "116903/116903 [==============================] - 19s 159us/step - loss: 4.5721 - accuracy: 0.2703 - val_loss: 6.3795 - val_accuracy: 0.2255\n",
      "Epoch 39/1000\n",
      "116903/116903 [==============================] - 19s 159us/step - loss: 4.5447 - accuracy: 0.2714 - val_loss: 6.3912 - val_accuracy: 0.2278\n",
      "Epoch 40/1000\n",
      "116903/116903 [==============================] - 19s 160us/step - loss: 4.5172 - accuracy: 0.2727 - val_loss: 6.4115 - val_accuracy: 0.2258\n",
      "Epoch 41/1000\n",
      "116903/116903 [==============================] - 18s 156us/step - loss: 4.4863 - accuracy: 0.2737 - val_loss: 6.4504 - val_accuracy: 0.2253\n",
      "Epoch 42/1000\n",
      "116903/116903 [==============================] - 19s 159us/step - loss: 4.4590 - accuracy: 0.2747 - val_loss: 6.4945 - val_accuracy: 0.2239\n",
      "Epoch 43/1000\n",
      "116903/116903 [==============================] - 19s 159us/step - loss: 4.4333 - accuracy: 0.2768 - val_loss: 6.5234 - val_accuracy: 0.2243\n",
      "Epoch 44/1000\n",
      "116903/116903 [==============================] - 18s 158us/step - loss: 4.4074 - accuracy: 0.2781 - val_loss: 6.5510 - val_accuracy: 0.2243\n",
      "Epoch 45/1000\n",
      "116903/116903 [==============================] - 18s 157us/step - loss: 4.3800 - accuracy: 0.2785 - val_loss: 6.5570 - val_accuracy: 0.2208\n",
      "Epoch 46/1000\n",
      "116903/116903 [==============================] - 18s 157us/step - loss: 4.3540 - accuracy: 0.2802 - val_loss: 6.6313 - val_accuracy: 0.2213\n",
      "Epoch 47/1000\n",
      "116903/116903 [==============================] - 18s 157us/step - loss: 4.3283 - accuracy: 0.2814 - val_loss: 6.6751 - val_accuracy: 0.2224\n",
      "Epoch 48/1000\n",
      "116903/116903 [==============================] - 19s 158us/step - loss: 4.3044 - accuracy: 0.2828 - val_loss: 6.6791 - val_accuracy: 0.2214\n",
      "Epoch 49/1000\n",
      "116903/116903 [==============================] - 19s 159us/step - loss: 4.2786 - accuracy: 0.2838 - val_loss: 6.7724 - val_accuracy: 0.2226\n",
      "Epoch 50/1000\n",
      "116903/116903 [==============================] - 19s 158us/step - loss: 4.2516 - accuracy: 0.2841 - val_loss: 6.7852 - val_accuracy: 0.2217\n",
      "Epoch 51/1000\n",
      "116903/116903 [==============================] - 19s 159us/step - loss: 4.2278 - accuracy: 0.2868 - val_loss: 6.8148 - val_accuracy: 0.2212\n",
      "Epoch 52/1000\n",
      "116903/116903 [==============================] - 19s 160us/step - loss: 4.1992 - accuracy: 0.2877 - val_loss: 6.8906 - val_accuracy: 0.2211\n",
      "Epoch 53/1000\n",
      "116903/116903 [==============================] - 18s 158us/step - loss: 4.1800 - accuracy: 0.2875 - val_loss: 6.8849 - val_accuracy: 0.2182\n",
      "Epoch 54/1000\n",
      "116903/116903 [==============================] - 19s 159us/step - loss: 4.1544 - accuracy: 0.2898 - val_loss: 6.9267 - val_accuracy: 0.2173\n",
      "Epoch 55/1000\n",
      "116903/116903 [==============================] - 19s 160us/step - loss: 4.1292 - accuracy: 0.2903 - val_loss: 6.9734 - val_accuracy: 0.2216\n",
      "Epoch 56/1000\n",
      "116903/116903 [==============================] - 19s 158us/step - loss: 4.1029 - accuracy: 0.2923 - val_loss: 7.0167 - val_accuracy: 0.2167\n",
      "Epoch 57/1000\n",
      "116903/116903 [==============================] - 18s 158us/step - loss: 4.0794 - accuracy: 0.2927 - val_loss: 7.0532 - val_accuracy: 0.2180\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(titles)):\n",
    "    TRAINING_MODEL_FOR_AUTHOR = i\n",
    "\n",
    "    print(\"generating training data for \" + titles[TRAINING_MODEL_FOR_AUTHOR])\n",
    "    #word_idx, idx_word, num_words, word_counts, features, labels = make_sequences(\n",
    "    #    formatted, TRAINING_LENGTH, lower=True)\n",
    "    filters = '%[\\\\]^_`{|}~\\t'\n",
    "    word_idx, idx_word, num_words, word_counts, abstracts, sequences, features, labels = make_sequences(formatted, TRAINING_LENGTH, lower=True, target=TRAINING_MODEL_FOR_AUTHOR)\n",
    "\n",
    "\n",
    "    X_train, X_valid, y_train, y_valid = create_train_valid(\n",
    "        features, labels, num_words)\n",
    "    X_train.shape\n",
    "    y_train.shape\n",
    "    import sys\n",
    "    def check_sizes(gb_min=1):\n",
    "        for x in globals():\n",
    "            size = sys.getsizeof(eval(x)) / 1e9\n",
    "            if size > gb_min:\n",
    "                print('Object:', x)\n",
    "                print('\\tSize (GB): ',size)\n",
    "    check_sizes(gb_min=.05)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    model = make_word_level_model(\n",
    "        num_words,\n",
    "        embedding_matrix,\n",
    "        lstm_cells=LSTM_CELLS,\n",
    "        trainable=True,\n",
    "        bi_direc=False,\n",
    "        lstm_layers=1)\n",
    "    model.summary()\n",
    "\n",
    "\n",
    "    model_name = titles[TRAINING_MODEL_FOR_AUTHOR]\n",
    "    model_dir = 'models/'\n",
    "\n",
    "    callbacks = make_callbacks(model_name)\n",
    "    model.compile(\n",
    "        optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=2048,\n",
    "        verbose=1,\n",
    "        epochs=1000,\n",
    "        callbacks=callbacks,\n",
    "        validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_evaluate(model_name, return_model=False):\n",
    "    \"\"\"Load in a trained model and evaluate with log loss and accuracy\"\"\"\n",
    "\n",
    "    model = load_model('{}{}.h5'.format(model_dir,model_name))\n",
    "    r = model.evaluate(X_valid, y_valid, batch_size=2048, verbose=1)\n",
    "\n",
    "    valid_crossentropy = r[0]\n",
    "    valid_accuracy = r[1]\n",
    "\n",
    "    print('Cross Entropy: {}'.format(round(valid_crossentropy, 4)))\n",
    "    print('Accuracy: {}%'.format(round(100 * valid_accuracy, 2)))\n",
    "\n",
    "    if return_model:\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50177/50177 [==============================] - 6s 117us/step\n",
      "Cross Entropy: 5.8587\n",
      "Accuracy: 22.26%\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "model_name = 'testModel3'\n",
    "model_dir=''\n",
    "model = load_and_evaluate(model_name, return_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 18441),\n",
       " ('\\n', 14409),\n",
       " ('the', 10461),\n",
       " ('and', 5546),\n",
       " ('of', 4458),\n",
       " ('i', 3002),\n",
       " ('to', 2290),\n",
       " ('.', 2025),\n",
       " ('in', 1946),\n",
       " ('you', 1680),\n",
       " ('a', 1390),\n",
       " ('with', 1300),\n",
       " ('!', 1294),\n",
       " ('is', 1196),\n",
       " ('all', 1100)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[array([',', '11.01'], dtype='<U5'),\n",
       " array(['\\n', '8.609'], dtype='<U5'),\n",
       " array(['the', '6.250'], dtype='<U5'),\n",
       " array(['and', '3.31'], dtype='<U4'),\n",
       " array(['of', '2.66'], dtype='<U4'),\n",
       " array(['i', '1.79'], dtype='<U4'),\n",
       " array(['to', '1.36'], dtype='<U4'),\n",
       " array(['.', '1.21'], dtype='<U4'),\n",
       " array(['in', '1.16'], dtype='<U4'),\n",
       " array(['you', '1.00'], dtype='<U4'),\n",
       " array(['a', '0.83'], dtype='<U4'),\n",
       " array(['with', '0.77'], dtype='<U4'),\n",
       " array(['!', '0.77'], dtype='<U4'),\n",
       " array(['is', '0.71'], dtype='<U4'),\n",
       " array(['all', '0.65'], dtype='<U4')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.random.seed(40)\n",
    "\n",
    "# Number of all words\n",
    "total_words = sum(word_counts.values())\n",
    "\n",
    "topWords = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "topWords\n",
    "a = []\n",
    "for i in topWords:  \n",
    "    a.append(np.asarray(i))\n",
    "for i in range(len(a)) :\n",
    "    a[i][1]= 100* (topWords[i][1]/total_words);\n",
    "a\n",
    "\n",
    "# Compute frequency of each word in vocab\n",
    "frequencies = [word_counts[word] / total_words for word in word_idx.keys()]\n",
    "frequencies.insert(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "\n",
    "def header(text, color='black'):\n",
    "    raw_html = '<h1 style=\"color: {};\"><center>'.format(color) + \\\n",
    "        str(text) + '</center></h1>'\n",
    "    return raw_html\n",
    "\n",
    "\n",
    "def box(text):\n",
    "    raw_html = '<div style=\"border:1px inset black;padding:1em;font-size: 20px;\">' + \\\n",
    "        str(text)+'</div>'\n",
    "    return raw_html\n",
    "\n",
    "\n",
    "def addContent(old_html, raw_html):\n",
    "    old_html += raw_html\n",
    "    return old_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def generate_output(model,\n",
    "                    sequences,\n",
    "                    training_length=50,\n",
    "                    new_words=50,\n",
    "                    diversity=1,\n",
    "                    return_output=False,\n",
    "                    n_gen=1):\n",
    "    \"\"\"Generate `new_words` words of output from a trained model and format into HTML.\"\"\"\n",
    "\n",
    "    # Choose a random sequence\n",
    "    seq = random.choice(sequences)\n",
    "\n",
    "    # Choose a random starting point\n",
    "    seed_idx = 3#random.randint(0, len(seq) - training_length - 1)\n",
    "    # Ending index for seed\n",
    "    end_idx = seed_idx + training_length\n",
    "\n",
    "    gen_list = []\n",
    "\n",
    "    for n in range(n_gen):\n",
    "        # Extract the seed sequence\n",
    "        seed = seq[seed_idx:end_idx]\n",
    "        print(seq)\n",
    "        original_sequence = [idx_word[i] for i in seed]\n",
    "        generated = seed[:] + ['#']\n",
    "\n",
    "        # Find the actual entire sequence\n",
    "        actual = generated[:] + seq[end_idx:end_idx + new_words]\n",
    "\n",
    "        # Keep adding new words\n",
    "        for i in range(new_words):\n",
    "\n",
    "            # Make a prediction from the seed\n",
    "            preds = model.predict(np.array(seed).reshape(1, -1))[0].astype(\n",
    "                np.float64)\n",
    "\n",
    "            # Diversify\n",
    "            preds = np.log(preds) / diversity\n",
    "            exp_preds = np.exp(preds)\n",
    "\n",
    "            # Softmax\n",
    "            preds = exp_preds / sum(exp_preds)\n",
    "\n",
    "            # Choose the next word\n",
    "            probas = np.random.multinomial(1, preds, 1)[0]\n",
    "\n",
    "            next_idx = np.argmax(probas)\n",
    "\n",
    "            # New seed adds on old word\n",
    "            seed = seed[1:] + [next_idx]\n",
    "            generated.append(next_idx)\n",
    "\n",
    "        # Showing generated and actual abstract\n",
    "        n = []\n",
    "\n",
    "        for i in generated:\n",
    "            n.append(idx_word.get(i, '< --- >'))\n",
    "\n",
    "        gen_list.append(n)\n",
    "\n",
    "    a = []\n",
    "\n",
    "    for i in actual:\n",
    "        a.append(idx_word.get(i, '< --- >'))\n",
    "\n",
    "    a = a[training_length:]\n",
    "\n",
    "    gen_list = [\n",
    "        gen[training_length:training_length + len(a)] for gen in gen_list\n",
    "    ]\n",
    "\n",
    "    if return_output:\n",
    "        return original_sequence, gen_list, a\n",
    "\n",
    "    # HTML formatting\n",
    "    seed_html = ''\n",
    "    seed_html = addContent(seed_html, header(\n",
    "        'Seed Sequence', color='darkblue'))\n",
    "    seed_html = addContent(seed_html,\n",
    "                           box(remove_spaces(' '.join(original_sequence))))\n",
    "\n",
    "    gen_html = ''\n",
    "    gen_html = addContent(gen_html, header('RNN Generated', color='darkred'))\n",
    "    gen_html = addContent(gen_html, box(remove_spaces(' '.join(gen_list[0]))))\n",
    "\n",
    "    a_html = ''\n",
    "    a_html = addContent(a_html, header('Actual', color='darkgreen'))\n",
    "    a_html = addContent(a_html, box(remove_spaces(' '.join(a))))\n",
    "\n",
    "    return seed_html, gen_html, a_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3555, 4, 4681, 6, 179, 7, 3, 230, 326, 13, 2, 1589, 1, 247, 1, 3, 109, 134, 16, 1, 2, 3, 73, 681, 1426, 134, 16, 1, 1590, 4682, 6, 4683, 13, 2, 396, 6, 1179, 20, 2845, 1, 6, 84, 67, 2845, 1, 2, 396, 6, 4684, 49, 59, 1, 3556, 49, 59, 1, 447, 133, 1, 2, 265, 12, 3557, 2846, 1, 1427, 1, 4685, 2847, 1, 2, 169, 4, 397, 6, 600, 3, 230, 326, 8, 2, 3, 6976, 14, 1591, 1, 2, 6, 62, 20, 1284, 3, 2848, 77, 1285, 1, 2, 6, 88, 31, 28, 330, 120, 70, 31, 28, 1, 2, 6, 88, 31, 4686, 18, 85, 75, 1086, 7, 38, 8, 2, 190, 65, 6, 813, 19, 55, 645, 2849, 1, 2, 6, 813, 38, 1, 86, 4, 6977, 813, 38, 12, 16, 764, 6, 118, 1, 2, 6, 508, 23, 14, 3558, 18, 16, 7, 1286, 4687, 5, 38, 1, 2, 6, 67, 448, 12, 38, 1, 4, 6, 52, 449, 38, 9, 287, 8, 2, 10, 326, 6, 600, 4, 151, 177, 13, 6, 362, 10, 28, 20, 15, 21, 14, 65, 13, 2, 6, 362, 21, 213, 433, 14, 210, 65, 8, 2, 65, 14, 3, 2850, 601, 5, 4688, 1, 646, 4689, 17, 3559, 39, 2, 3, 323, 12, 41, 3560, 188, 1, 3, 2851, 1, 3, 6978, 1, 3, 2852, 363, 1, 28, 20, 2853, 1, 2, 3, 545, 1, 3, 3561, 113, 3, 2392, 1, 3, 4690, 1592, 1, 3, 3562, 3563, 1, 3, 1017, 3564, 5, 1018, 1, 2, 3, 2051, 409, 1, 3, 450, 4691, 2393, 1, 3, 4692, 1, 3, 4693, 1796, 1, 2, 3, 509, 4694, 1, 3, 2854, 1, 3, 581, 5, 2855, 315, 3, 1593, 1, 3, 287, 162, 26, 3, 1593, 1, 2, 31, 157, 1, 6, 210, 157, 1, 77, 241, 875, 1, 227, 146, 30, 4695, 1, 2, 227, 37, 28, 1428, 1, 227, 37, 28, 242, 7, 16, 8, 2, 10, 130, 21, 1594, 16, 12, 423, 7, 451, 13, 2, 10, 647, 21, 259, 26, 4696, 19, 1087, 4, 115, 38, 380, 13, 2, 10, 174, 21, 4697, 16, 4, 15, 184, 9, 510, 2052, 1429, 13, 2, 10, 1430, 1595, 9, 3, 2394, 2856, 32, 3, 6979, 13, 2, 6, 125, 10, 28, 628, 12, 336, 6980, 28, 56, 242, 7, 16, 8, 2, 10, 4698, 546, 5, 3, 206, 13, 10, 169, 3565, 34, 3, 2857, 13, 2, 10, 2858, 13, 10, 2053, 4, 1797, 5, 814, 13, 10, 4699, 1088, 13, 10, 424, 233, 13, 2, 10, 1287, 5, 356, 13, 10, 6981, 6982, 13, 10, 1596, 13, 2, 10, 4700, 4, 2859, 13, 10, 4701, 4, 582, 3566, 13, 2, 10, 949, 465, 876, 1180, 511, 1431, 56, 167, 13, 2, 10, 715, 4, 950, 815, 13, 10, 2860, 13, 2, 10, 6983, 1181, 5, 1019, 1597, 13, 10, 4702, 4703, 13, 2, 26, 15, 21, 99, 224, 281, 10, 6, 362, 10, 43, 4704, 7, 2054, 1, 4, 50, 135, 4705, 3, 137, 2395, 7, 16, 1, 2, 26, 3, 288, 4, 3, 144, 6, 125, 10, 43, 4706, 42, 816, 2055, 1, 4, 3, 1798, 2396, 135, 30, 4707, 4, 4708, 12, 16, 8, 2, 3, 71, 1432, 434, 117, 4, 337, 117, 1, 2, 3, 1433, 648, 1, 87, 219, 9, 69, 225, 174, 1, 2, 3, 357, 466, 9, 70, 23, 14, 1598, 1, 4, 1599, 70, 23, 14, 20, 1598, 1, 2, 3, 1434, 163, 5, 3, 547, 6984, 1799, 452, 2056, 5, 3, 326, 8, 2, 29, 4709, 6, 600, 13, 29, 547, 326, 13, 62, 10, 197, 7, 16, 1, 62, 20, 309, 16, 25, 2, 62, 10, 197, 1, 2397, 20, 6985, 10, 309, 16, 10, 28, 358, 25, 2, 62, 10, 197, 1, 6, 67, 716, 6986, 67, 120, 4710, 4, 6987, 7, 16, 25, 2, 29, 547, 326, 13, 6, 197, 162, 6, 67, 20, 2398, 7, 309, 6988, 6, 63, 10, 1, 2, 10, 2057, 16, 435, 72, 6, 146, 2057, 84, 1, 2, 10, 57, 30, 59, 7, 16, 72, 19, 717, 8, 2, 6, 125, 1800, 629, 131, 15, 4711, 9, 3, 230, 130, 1, 2, 6, 125, 6, 270, 512, 65, 84, 4, 62, 1435, 1, 2, 6, 125, 342, 6, 467, 27, 3, 326, 6, 57, 110, 1, 4, 302, 4712, 16, 57, 110, 16, 1, 2, 6, 125, 302, 6, 40, 183, 30, 513, 8, 2, 26, 51, 310, 1, 468, 13, 2, 26, 51, 310, 6, 4713, 84, 6989, 5, 4714, 4, 4715, 602, 13, 2, 436, 70, 6, 6990, 105, 682, 1, 3567, 4, 1600, 1, 2, 1288, 7, 153, 1, 2861, 120, 33, 31, 197, 1, 2, 2058, 1, 2399, 1, 2059, 1, 4716, 8, 2, 1601, 37, 12, 2862, 52, 4717, 84, 5, 3, 526, 21, 135, 346, 16, 8, 2, 6, 2863, 95, 4718, 5, 130, 1, 2, 3, 410, 4, 3, 271, 28, 202, 1, 4, 3, 248, 4, 3, 282, 28, 202, 8, 2, 6, 67, 2060, 72, 6, 173, 13, 2, 6, 437, 20, 88, 6, 718, 56, 167, 2061, 13, 2, 15, 817, 186, 7, 16, 1, 2, 6, 146, 2864, 82, 7, 86, 4, 138, 1, 10, 43, 265, 191, 121, 7, 16, 1, 6, 135, 62, 3, 137, 7, 10, 1, 2, 6, 52, 4719, 18, 84, 4, 10, 24, 6, 118, 1, 2, 6, 52, 2400, 84, 316, 86, 4, 138, 24, 6, 118, 1, 2, 6, 52, 2062, 11, 148, 2401, 4, 3568, 316, 38, 1, 2, 302, 3569, 16, 23, 57, 20, 2402, 16, 1, 2, 302, 2865, 16, 47, 17, 128, 57, 30, 3570, 4, 57, 4720, 16, 8, 2, 50, 102, 11, 319, 192, 86, 131, 7, 514, 1, 23, 135, 20, 2866, 16, 1, 2, 50, 102, 11, 319, 186, 438, 5, 138, 1289, 1, 23, 135, 20, 2403, 16, 8, 2, 50, 6, 40, 3, 877, 5, 3, 630, 5, 3, 225, 364, 1, 2, 23, 14, 7, 765, 9, 3, 230, 130, 1, 4, 1801, 4, 260, 12, 3, 71, 8, 2, 65, 11, 95, 1802, 1436, 99, 603, 1, 2, 35, 191, 11, 1436, 2867, 96, 3, 766, 5, 3, 289, 272, 5, 86, 1, 2, 69, 3571, 5, 818, 4, 52, 4721, 453, 4, 1803, 15, 2868, 4, 15, 1804, 365, 23, 8, 36, 2, 65, 14, 3, 2404, 5, 1089, 1, 2, 1089, 14, 20, 878, 4722, 9, 1602, 1, 2, 1089, 214, 30, 398, 26, 45, 604, 23, 7, 266, 20, 604, 23, 1, 2, 1089, 14, 5, 3, 68, 1, 14, 20, 4723, 5, 951, 1, 14, 69, 105, 951, 1, 2, 4724, 7, 15, 2869, 4, 647, 4, 1603, 4, 14, 397, 1, 2, 14, 3, 2870, 5, 3, 1020, 4, 1021, 5, 184, 1, 4, 3, 4725, 5, 184, 39, 2, 213, 53, 14, 9, 3, 649, 5, 3, 347, 5, 184, 21, 3572, 23, 54, 5, 3, 68, 8, 2, 50, 6, 4726, 2871, 4, 1437, 1, 2, 31, 154, 952, 120, 9, 3573, 1, 60, 20, 952, 34, 15, 159, 3, 2405, 366, 4, 228, 3, 1022, 4, 482, 2063, 8, 2, 65, 14, 2406, 1, 2, 65, 14, 11, 78, 6991, 4727, 65, 33, 47, 99, 9, 81, 1, 2, 3, 164, 1, 3, 297, 1, 2872, 1, 6992, 31, 28, 1438, 5, 10, 1, 10, 28, 1438, 5, 38, 8, 2, 97, 3, 4728, 5, 87, 650, 4729, 39, 2, 70, 14, 47, 75, 439, 234, 3, 3574, 18, 10, 4, 16, 25, 2, 70, 14, 47, 21, 4730, 4731, 4, 4732, 18, 10, 4, 16, 25, 2, 65, 14, 2407, 1, 23, 14, 20, 2873, 3575, 1, 23, 14, 4733, 39, 2, 62, 10, 88, 33, 23, 14, 24, 10, 157, 7, 30, 399, 32, 1805, 25, 2, 62, 10, 88, 3, 631, 5, 85, 819, 6993, 25, 2, 65, 14, 3, 1806, 5, 3, 68, 1, 2, 3, 1806, 5, 3, 68, 252, 26, 262, 61, 3576, 1290, 1, 101, 4734, 1291, 8, 2, 48, 3577, 290, 28, 31, 25, 48, 381, 9, 3, 411, 290, 28, 31, 25, 2, 290, 28, 53, 86, 4, 138, 21, 149, 31, 28, 879, 16, 3, 2408, 2874, 19, 221, 25, 2, 290, 111, 31, 309, 16, 62, 19, 1182, 5, 185, 2064, 1439, 4, 3578, 25, 2, 290, 28, 53, 267, 6, 114, 253, 159, 37, 291, 4, 2065, 381, 1183, 96, 16, 25, 2, 35, 6, 125, 31, 1090, 53, 651, 4, 412, 27, 85, 267, 4, 1184, 1023, 1024, 24, 6, 157, 39, 36, 2, 33, 14, 23, 6, 2875, 56, 880, 12, 1805, 25, 2, 33, 12, 93, 2409, 24, 6, 2066, 27, 3, 3579, 32, 41, 207, 25, 2, 33, 12, 93, 3580, 3581, 41, 3582, 32, 3, 263, 24, 6, 253, 32, 4, 1025, 25, 2, 33, 719, 16, 7, 30, 247, 7, 11, 953, 4, 425, 2876, 25, 33, 719, 38, 7, 30, 247, 7, 202, 25, 2, 3, 1806, 5, 3, 68, 14, 720, 1, 65, 14, 720, 8, 2, 6, 125, 23, 3583, 3, 230, 130, 1, 440, 34, 15, 413, 1, 2, 50, 23, 1807, 2067, 126, 1, 46, 28, 4735, 1292, 8, 2, 65, 1091, 3, 881, 4, 2068, 1185, 1, 2, 3, 881, 4, 2068, 1185, 14, 3, 1293, 4, 2069, 5, 78, 4, 181, 1, 2, 35, 3, 2877, 5, 3, 454, 3584, 49, 3585, 4, 1604, 87, 89, 54, 5, 3, 820, 5, 222, 1, 72, 23, 1808, 452, 4, 216, 882, 54, 5, 178, 8, 36, 2, 170, 3, 881, 4, 2068, 1185, 3586, 3, 1294, 5, 3, 63, 5, 123, 4, 55, 1, 2, 26, 23, 821, 2878, 3, 1809, 21, 1803, 343, 4, 4736, 1, 2, 170, 23, 2879, 3, 1605, 2410, 2880, 5, 1810, 8, 2, 652, 13, 302, 10, 28, 92, 600, 12, 16, 8, 2, 3587, 12, 16, 10, 426, 33, 114, 2070, 8, 2, 3, 71, 114, 2070, 1, 2, 3, 71, 14, 632, 1, 211, 1, 2881, 34, 235, 1, 275, 14, 632, 4, 2881, 34, 235, 1, 2, 30, 20, 2882, 1, 427, 27, 1, 53, 28, 215, 184, 120, 1811, 1, 2, 6, 508, 7, 10, 53, 28, 215, 184, 59, 186, 72, 155, 146, 254, 8, 2, 652, 13, 46, 183, 20, 512, 65, 1, 2, 683, 216, 48, 4737, 1186, 1, 683, 4738, 51, 1812, 46, 214, 515, 65, 1, 2, 683, 3588, 51, 883, 4, 683, 348, 48, 292, 46, 183, 20, 1295, 65, 1, 2, 683, 327, 3, 3589, 21, 4739, 126, 46, 28, 2883, 7, 822, 23, 37, 11, 152, 149, 8, 2, 652, 13, 3, 4740, 57, 30, 483, 1, 2, 46, 52, 367, 4741, 4, 328, 400, 1, 2, 46, 52, 118, 70, 359, 455, 1, 217, 1440, 1, 4, 3, 1441, 2071, 2072, 32, 159, 193, 367, 8, 2, 652, 13, 12, 401, 1, 653, 1, 3, 71, 1, 3, 1813, 1, 2, 527, 1, 1442, 1, 4742, 1, 2411, 1, 2884, 39, 2, 652, 13, 26, 15, 3590, 13, 2, 26, 42, 3590, 1, 29, 4743, 4, 4744, 767, 8, 2, 3, 2885, 4745, 3591, 91, 3, 6994, 1187, 456, 49, 528, 8, 2, 652, 13, 60, 179, 1188, 13, 2, 47, 3587, 12, 16, 2073, 3, 225, 221, 1, 4746, 1, 2412, 1, 2, 227, 154, 92, 7, 3, 1296, 189, 47, 17, 128, 344, 1443, 4, 527, 1, 2, 92, 20, 65, 102, 10, 43, 716, 4747, 3, 225, 5, 236, 1, 2, 97, 85, 154, 92, 75, 92, 9, 216, 4, 6995, 548, 1, 2, 49, 1606, 363, 1, 49, 4748, 6996, 17, 3592, 3593, 14, 2883, 65, 8, 2, 35, 6, 4, 202, 62, 20, 2074, 32, 2886, 1, 4749, 1, 1814, 1, 2, 46, 2074, 32, 139, 2413, 8, 36, 2, 684, 13, 6, 52, 30, 2414, 12, 10, 1, 2, 6, 62, 20, 1444, 3, 55, 2887, 2415, 1, 37, 1444, 2888, 148, 2415, 1, 2, 48, 28, 3, 203, 21, 183, 2075, 7, 10, 320, 2, 10, 57, 20, 4750, 91, 33, 14, 823, 1189, 320, 2, 10, 57, 2400, 12, 2416, 117, 15, 21, 10, 3594, 17, 2889, 1, 2, 10, 37, 1607, 34, 3, 160, 7, 156, 10, 131, 1815, 1, 10, 1190, 1816, 236, 7, 1817, 134, 10, 28, 823, 32, 106, 2076, 259, 7, 605, 1, 2, 10, 57, 30, 2890, 7, 3, 1818, 2417, 4, 3595, 5, 85, 75, 515, 324, 10, 1, 2, 33, 4751, 5, 63, 10, 822, 10, 57, 97, 606, 12, 824, 884, 5, 1191, 1, 2, 10, 57, 20, 2418, 3, 346, 5, 85, 75, 469, 44, 1445, 226, 170, 10, 8, 2, 652, 13, 113, 3, 95, 885, 1, 4, 7, 1086, 7, 38, 13, 2, 31, 142, 28, 27, 3, 6997, 28, 3, 529, 4, 954, 6998, 28, 3, 721, 138, 1, 2, 4752, 5, 4753, 5, 400, 4, 1026, 5, 400, 1, 2, 825, 5, 108, 11, 231, 1, 4754, 5, 108, 11, 2891, 5, 129, 1, 2, 4755, 5, 108, 424, 2419, 1, 4755, 5, 143, 424, 1819, 1, 2, 4756, 5, 86, 4, 138, 1, 4757, 5, 206, 1, 768, 4758, 1, 2, 4759, 4, 4760, 5, 3596, 1, 955, 1, 1180, 5, 3, 263, 1, 2, 1608, 34, 4761, 1, 4762, 5, 3597, 1, 722, 4763, 5, 220, 1, 3598, 5, 220, 1, 2, 329, 5, 3599, 1, 4764, 32, 3600, 607, 1, 4765, 5, 2892, 1, 2, 1092, 82, 4766, 956, 1, 82, 3, 145, 1, 3, 336, 145, 107, 957, 26, 21, 156, 3601, 23, 1, 2, 1092, 24, 12, 885, 1, 2077, 44, 105, 1093, 3602, 1, 2, 4767, 26, 3, 628, 4768, 4769, 1, 2, 1092, 3603, 12, 44, 105, 409, 1, 1092, 12, 44, 1297, 4, 4770, 1298, 1, 2, 1092, 12, 44, 1446, 1, 583, 1, 3604, 1, 397, 1, 2, 1092, 12, 44, 105, 2893, 55, 321, 1, 5, 1298, 17, 1446, 1, 2, 55, 321, 1, 348, 1, 4771, 1, 402, 12, 3, 1192, 2894, 5, 3, 382, 1, 2, 55, 321, 1, 482, 247, 12, 3, 645, 4772, 468, 5, 94, 8, 2, 652, 13, 7, 21, 156, 14, 441, 24, 23, 83, 2895, 1, 2, 7, 4773, 167, 1, 4774, 5, 203, 1, 2896, 5, 826, 1, 2, 7, 1094, 15, 9, 3, 600, 31, 1447, 7, 1, 4, 3, 203, 4, 826, 31, 1447, 7, 1, 2, 100, 7, 1094, 38, 9, 3, 827, 5, 1448, 2078, 1, 2, 7, 40, 133, 1193, 37, 33, 10, 154, 958, 23, 4, 157, 23, 1, 2, 7, 2079, 49, 98, 1, 683, 424, 1, 37, 33, 10, 154, 958, 23, 4, 157, 23, 1, 2, 7, 151, 91, 17, 124, 3, 326, 37, 23, 1820, 4, 456, 18, 10, 1, 683, 73, 37, 23, 1820, 4, 456, 18, 10, 1, 2, 7, 40, 49, 828, 1, 20, 1095, 17, 77, 1, 37, 10, 210, 118, 1096, 1, 2, 7, 40, 49, 2080, 37, 154, 1449, 23, 1, 1821, 15, 208, 6999, 17, 2420, 1, 2897, 3, 2081, 60, 20, 2897, 45, 2082, 5, 23, 1, 2, 7, 179, 3, 225, 5, 3, 2421, 1299, 4, 3, 450, 425, 3605, 4775, 1, 4, 3, 1450, 4776, 5, 3, 2898, 1796, 1, 4, 3, 959, 5, 1027, 4, 723, 5, 2899, 1, 2, 7, 179, 7, 42, 549, 54, 5, 3, 608, 206, 24, 10, 157, 61, 1, 2, 7, 813, 1822, 4, 303, 12, 10, 1028, 4682, 10, 118, 1, 2, 7, 654, 3, 3606, 5, 86, 54, 5, 44, 1823, 24, 10, 4777, 38, 1, 7, 654, 3, 63, 54, 5, 44, 766, 8, 2, 7, 179, 42, 388, 27, 3, 326, 12, 10, 1, 18, 15, 21, 10, 309, 38, 324, 10, 1, 2, 7, 88, 3, 382, 178, 24, 11, 326, 1, 24, 108, 724, 1, 24, 724, 18, 3587, 530, 8, 2, 15, 633, 180, 18, 3, 1300, 5, 530, 1, 2, 15, 1029, 1, 15, 531, 184, 1, 960, 1, 7000, 21, 83, 17, 14, 2900, 96, 51, 368, 17, 77, 368, 1, 821, 315, 3607, 4, 2901, 134, 3, 550, 5, 530, 228, 3, 609, 724, 5, 3, 382, 8, 2, 5, 3, 1300, 5, 3, 530, 5, 86, 4, 138, 228, 3, 609, 724, 5, 3, 382, 1, 15, 140, 1300, 14, 3, 1097, 1609, 4, 2083, 8, 2, 304, 648, 1, 304, 470, 1, 2, 829, 1, 551, 1, 584, 1, 1610, 1, 725, 1, 1098, 1, 1030, 1, 2902, 1, 1301, 1, 2, 886, 1, 471, 1, 2422, 1, 552, 1, 1428, 32, 86, 1, 2084, 32, 86, 1, 2, 31, 118, 13, 31, 118, 13, 6, 88, 21, 31, 118, 1, 37, 6, 88, 20, 70, 31, 118, 1, 2, 37, 6, 88, 21, 31, 118, 170, 3, 7001, 213, 95, 8, 2, 302, 10, 28, 1, 92, 158, 13, 17, 78, 17, 181, 92, 158, 13, 2, 10, 183, 20, 1302, 887, 4, 2423, 53, 9, 3, 255, 1, 472, 10, 1303, 23, 1, 17, 472, 23, 99, 224, 1303, 18, 10, 8, 2, 54, 5, 3, 305, 4778, 13, 54, 26, 324, 3, 3608, 13, 2, 23, 14, 2085, 7, 4779, 1, 6, 88, 15, 4, 1431, 23, 8, 2, 249, 61, 10, 24, 888, 24, 3, 127, 1, 2, 61, 3, 830, 1, 1824, 1, 4780, 1, 4781, 5, 331, 1, 2, 1031, 5, 2903, 4, 2904, 1, 1031, 5, 85, 1825, 4, 4782, 273, 1, 2, 249, 11, 877, 211, 4783, 4, 1611, 8, 2, 49, 726, 1, 49, 484, 1, 49, 332, 1, 4784, 7, 112, 3, 1826, 1, 2, 266, 1451, 1, 11, 2905, 5, 87, 45, 1, 4785, 4, 2424, 23, 369, 1, 2, 1827, 4, 2906, 61, 3, 303, 5, 3, 206, 1, 3609, 4, 4786, 9, 3, 7002, 1, 2, 9, 3, 2086, 5, 1828, 1, 9, 2425, 1, 9, 3, 547, 2426, 1, 2, 317, 7, 3, 356, 5, 86, 4, 138, 1, 34, 3, 1612, 1, 9, 3, 4787, 1, 610, 1, 2, 3610, 3611, 1, 1304, 2427, 1, 414, 2907, 1, 94, 159, 3, 3612, 1, 1099, 159, 3, 4788, 1, 2, 159, 3, 2908, 4, 3613, 1, 159, 3, 4789, 4, 1829, 723, 1, 2, 3614, 634, 12, 3, 1830, 1, 1613, 20, 11, 4790, 5, 178, 8, 2, 1613, 5, 2909, 283, 1, 37, 114, 5, 178, 8, 2, 652, 13, 61, 1305, 4, 457, 13, 2, 3, 3615, 21, 83, 2428, 214, 30, 4791, 8, 2, 43, 3, 164, 1305, 2429, 25, 2, 33, 99, 2429, 25, 236, 25, 42, 769, 25, 275, 25, 2, 50, 473, 16, 7003, 14, 889, 9, 3, 1614, 5, 184, 21, 26, 77, 2910, 5, 1831, 1, 49, 611, 33, 1, 57, 92, 158, 213, 7, 175, 11, 483, 1452, 2087, 8, 2, 19, 259, 14, 3, 259, 5, 3, 494, 1, 6, 2430, 2431, 4792, 1, 2, 47, 436, 12, 16, 183, 118, 120, 890, 1, 2, 47, 436, 12, 16, 369, 495, 12, 2432, 2433, 1, 1832, 1, 891, 1615, 1, 4793, 8, 2, 652, 13, 3, 326, 14, 134, 126, 13, 2, 23, 14, 7004, 43, 1194, 7005, 105, 318, 43, 1194, 23, 7006, 20, 2911, 13, 2, 168, 3, 2434, 515, 27, 3, 1833, 3616, 1, 4, 3, 243, 27, 3, 3617, 4794, 13, 2, 168, 3, 2088, 515, 9, 3, 3618, 13, 168, 3, 892, 515, 4795, 13, 2, 168, 3, 893, 198, 13, 415, 20, 3, 727, 5, 3, 1306, 13, 2, 168, 3, 2435, 4796, 9, 41, 3619, 13, 168, 3, 2089, 3620, 9, 3, 1834, 1, 4, 3, 1307, 2912, 3, 383, 8, 2, 1308, 1, 6, 52, 115, 10, 19, 117, 13, 2, 6, 115, 10, 19, 63, 59, 831, 72, 892, 1, 2, 6, 115, 10, 84, 134, 3621, 17, 383, 39, 2, 52, 10, 115, 16, 236, 25, 52, 10, 92, 600, 12, 16, 25, 2, 57, 46, 2436, 32, 107, 140, 24, 73, 24, 46, 585, 25, 2, 1453, 1616, 8, 2, 403, 3622, 2, 23, 183, 20, 30, 7007, 21, 11, 403, 1617, 1, 24, 93, 135, 43, 126, 1195, 1, 14, 655, 11, 435, 17, 1618, 261, 5, 2090, 3, 485, 8, 53, 28, 108, 1100, 5, 2090, 1022, 1309, 24, 121, 39, 4, 227, 59, 4797, 1, 9, 3623, 5, 7008, 7009, 1, 72, 26, 11, 7010, 1196, 8, 37, 1022, 27, 11, 403, 1617, 14, 1309, 7011, 8, 47, 75, 14, 360, 5, 3, 2437, 218, 20, 894, 9, 3624, 5, 3, 2091, 1, 37, 5, 442, 4798, 7012, 3, 1032, 4, 298, 12, 156, 3, 370, 2092, 34, 454, 1, 4, 3, 237, 4, 586, 4799, 5, 3, 7013, 127, 8, 47, 214, 254, 587, 47, 1310, 41, 2438, 27, 1, 17, 1033, 23, 234, 1, 12, 59, 1311, 8, 3, 3625, 5, 3, 3626, 1310, 81, 9, 2439, 18, 21, 5, 3, 4800, 8, 342, 47, 218, 14, 20, 97, 11, 4801, 9, 178, 1, 37, 52, 30, 1619, 7014, 9, 3, 7015, 39, 4, 56, 728, 2913, 27, 7, 728, 9, 106, 441, 2440, 8, 23, 14, 51, 21, 56, 404, 146, 473, 39, 31, 52, 1454, 30, 196, 4802, 17, 196, 34, 1455, 832, 106, 310, 39, 31, 62, 20, 458, 234, 3, 45, 365, 3, 140, 1, 1835, 15, 89, 18, 3, 895, 1, 4, 15, 895, 18, 3, 961, 89, 8, 4, 1, 349, 15, 1, 23, 14, 65, 21, 42, 7016, 4803, 5, 4804, 8, 41, 238, 1091, 365, 85, 75, 1101, 44, 7017, 9, 7018, 4805, 1, 111, 47, 311, 146, 7019, 23, 9, 11, 681, 7020, 8, 47, 52, 20, 362, 21, 3, 4806, 14, 59, 510, 9, 3, 3627, 7021, 8, 47, 52, 20, 362, 21, 7, 253, 51, 7022, 833, 14, 655, 7, 7023, 4, 7024, 311, 1, 4, 92, 7, 41, 3628, 1, 34, 79, 1, 12, 11, 2093, 5, 4807, 27, 41, 1455, 4808, 1, 4, 11, 7025, 79, 5, 411, 9, 41, 298, 8, 20, 18, 81, 3, 3629, 4809, 895, 5, 3, 3630, 4810, 13, 47, 99, 133, 337, 5, 78, 37, 11, 3631, 447, 18, 7026, 4, 11, 1836, 7027, 39, 4, 244, 41, 1837, 1, 102, 47, 30, 11, 7028, 1, 52, 30, 7029, 4, 7030, 8, 23, 14, 3, 1838, 5, 191, 106, 45, 7, 179, 4811, 24, 167, 2402, 24, 14, 1097, 7, 7031, 720, 1, 4, 3632, 3, 720, 9, 3, 588, 39, 47, 14, 3, 78, 5, 3, 7032, 1, 9, 1102, 1, 75, 369, 1619, 4, 7033, 1618, 8, 2, 50, 1, 7, 30, 4812, 7034, 1, 11, 403, 1617, 293, 30, 350, 96, 165, 8, 102, 10, 118, 9, 11, 1312, 1, 17, 244, 9, 7035, 1, 23, 14, 49, 528, 11, 403, 1617, 9, 2909, 37, 306, 39, 23, 14, 213, 283, 4, 59, 9, 3, 275, 5, 11, 7036, 8, 11, 403, 1617, 293, 30, 350, 96, 165, 1, 685, 468, 14, 5, 3, 1614, 39, 685, 10, 293, 30, 1839, 7, 512, 4, 118, 27, 1, 4, 516, 51, 261, 17, 21, 1, 24, 3, 7037, 1033, 10, 39, 4, 685, 10, 183, 43, 42, 105, 2441, 1, 4, 646, 4813, 7038, 11, 7039, 4810, 1, 58, 7040, 9, 98, 12, 11, 1034, 8, 4, 104, 10, 183, 30, 230, 7, 15, 7041, 4, 168, 42, 381, 179, 7042, 26, 33, 10, 40, 8, 10, 293, 30, 24, 11, 1837, 18, 77, 389, 7, 458, 96, 8, 612, 6, 214, 40, 3, 4814, 1, 612, 1103, 2914, 1, 612, 5, 403, 4, 1104, 34, 3, 137, 98, 8, 111, 6, 67, 9, 3, 485, 1, 6, 834, 7, 7043, 110, 3, 485, 1, 612, 156, 14, 3, 4815, 5, 15, 21, 146, 30, 496, 96, 3, 611, 8, 53, 293, 30, 49, 7044, 5, 390, 34, 42, 7045, 1, 7, 7046, 27, 3, 7047, 656, 5, 3, 454, 8, 4, 56, 73, 24, 11, 78, 14, 3633, 47, 214, 3634, 311, 7, 21, 1035, 7048, 21, 252, 5, 167, 835, 9, 3, 230, 130, 1, 21, 2092, 9, 11, 2093, 5, 2094, 4, 7049, 5, 3, 517, 1, 4, 3635, 9, 11, 237, 21, 875, 4804, 8, 2, 2915, 3, 235, 89, 17, 56, 5, 77, 1617, 53, 28, 2442, 5, 4816, 1, 111, 3, 4817, 1840, 59, 72, 4818, 1105, 41, 2438, 1, 111, 47, 14, 657, 9, 11, 415, 7, 2443, 23, 4819, 82, 3, 7050, 4, 1, 110, 3636, 27, 11, 7051, 3637, 1, 612, 115, 532, 3638, 4, 118, 27, 239, 8, 612, 4, 60, 23, 325, 7052, 11, 7053, 5, 7054, 8, 23, 1106, 2444, 39, 3, 298, 5, 3, 836, 3639, 315, 23, 8, 4, 49, 1620, 43, 10, 7055, 3, 3640, 82, 42, 1841, 72, 3, 7056, 5, 260, 28, 7057, 26, 10, 1, 10, 3641, 236, 32, 3, 261, 8, 11, 152, 770, 27, 1, 4, 23, 14, 24, 110, 24, 20, 47, 52, 1197, 7, 132, 8, 4, 120, 18, 81, 1, 7058, 81, 7, 30, 49, 95, 682, 9, 21, 589, 1, 102, 47, 7059, 771, 49, 7060, 4820, 34, 11, 2445, 39, 18, 27, 191, 106, 3637, 1, 6, 7061, 88, 156, 14, 3, 59, 4821, 1, 17, 587, 23, 14, 1618, 7, 3642, 3, 2446, 5, 42, 4822, 17, 3, 7062, 2916, 5, 42, 7063, 8, 11, 7064, 2917, 1, 7065, 1, 2918, 1, 7, 3, 384, 4823, 962, 5, 3, 553, 1592, 1, 146, 9, 49, 1198, 4824, 7, 178, 3, 4742, 5, 48, 7066, 8, 6, 686, 45, 78, 75, 83, 4825, 24, 11, 2919, 4826, 1, 685, 1, 3643, 11, 2095, 363, 12, 11, 268, 1107, 1, 47, 7067, 24, 47, 896, 110, 11, 256, 8, 4, 10, 135, 30, 7068, 102, 6, 131, 7, 254, 10, 15, 3, 486, 4, 4827, 687, 75, 43, 7069, 7, 16, 21, 1, 111, 27, 403, 3622, 1, 31, 7070, 1621, 330, 7071, 294, 11, 7072, 5, 268, 635, 111, 1, 24, 7073, 349, 1, 3, 7074, 4820, 7075, 315, 44, 307, 26, 276, 11, 2445, 8, 4, 65, 1, 3644, 10, 125, 6, 67, 7076, 1, 14, 4828, 105, 1826, 1, 26, 41, 4829, 612, 27, 436, 11, 836, 1, 612, 156, 14, 56, 121, 21, 53, 293, 30, 11, 7077, 7078, 27, 15, 75, 43, 20, 688, 23, 7079, 2, 612, 115, 16, 3, 312, 487, 240, 82, 19, 188, 1, 612, 1103, 47, 1, 612, 4, 3, 416, 3645, 613, 19, 318, 1, 11, 1313, 326, 134, 16, 1, 4, 11, 532, 636, 141, 370, 7, 7080, 104, 7, 1456, 13, 23, 14, 1199, 102, 6, 214, 827, 93, 1842, 27, 48, 2447, 7081, 8, 6, 897, 1, 6, 729, 1, 6, 1843, 1, 6, 132, 18, 185, 8, 612, 2, 4830, 13, 113, 21, 4831, 5, 19, 332, 12, 3, 3646, 1, 10, 135, 20, 43, 3647, 1, 135, 10, 1, 7, 3648, 21, 9, 3, 235, 363, 25, 37, 46, 43, 49, 7082, 4832, 1, 4, 1, 244, 9, 837, 1, 183, 15, 7083, 7, 30, 24, 1457, 4, 1200, 24, 139, 7084, 8, 23, 83, 20, 56, 12, 2914, 8, 4, 1458, 74, 4827, 47, 14, 35, 24, 1, 360, 1, 3649, 3, 4829, 36, 9, 3, 1459, 5, 403, 3622, 8, 47, 14, 227, 5, 42, 1844, 86, 9, 1314, 4833, 1, 75, 253, 44, 2096, 832, 11, 89, 320, 532, 636, 141, 370, 14, 41, 1036, 8, 4, 104, 47, 183, 43, 11, 1313, 326, 1, 3, 7085, 13, 2, 60, 53, 14, 45, 241, 6, 650, 7, 9, 48, 155, 5, 41, 1, 45, 241, 9, 3, 95, 4834, 3650, 21, 817, 7, 16, 20, 2920, 1198, 8, 6, 62, 20, 7086, 5, 21, 1622, 4, 689, 8, 772, 5, 48, 1460, 3, 4835, 39, 31, 772, 1315, 91, 3, 517, 54, 5, 69, 2921, 4836, 2446, 39, 4, 31, 772, 1316, 3, 2441, 8, 4837, 403, 14, 20, 56, 7087, 7, 3, 119, 1, 4, 23, 7088, 4, 7089, 3, 415, 8, 7090, 1, 111, 277, 10, 43, 773, 315, 106, 2052, 3651, 1, 23, 7091, 49, 3652, 173, 26, 10, 7, 427, 23, 91, 1, 4, 60, 23, 7092, 10, 26, 1456, 7093, 5, 2909, 283, 8, 110, 4838, 1, 110, 3, 199, 5, 11, 7094, 4839, 1, 23, 7095, 7096, 4, 2448, 7, 260, 3, 7097, 7098, 5, 3, 415, 8, 46, 146, 125, 5, 51, 17, 21, 1, 730, 4, 4840, 1, 24, 11, 256, 3653, 1, 17, 24, 46, 125, 9, 11, 454, 7099, 39, 46, 146, 175, 7100, 17, 2922, 54, 7101, 1, 4, 4841, 9, 11, 319, 1100, 12, 155, 17, 1814, 39, 37, 111, 23, 252, 7, 2414, 199, 1, 111, 46, 92, 7, 654, 731, 299, 18, 106, 4842, 1, 46, 154, 278, 3, 3654, 24, 554, 4, 73, 24, 46, 1623, 39, 3, 95, 4843, 5, 3, 415, 52, 20, 7102, 7, 3, 4844, 1, 37, 361, 1, 107, 45, 1, 34, 317, 1, 7103, 41, 226, 82, 41, 105, 417, 4, 2097, 27, 41, 105, 2923, 173, 13, 2, 9, 3, 732, 5, 11, 1461, 253, 1, 10, 40, 1, 53, 14, 167, 7104, 9, 3, 2924, 8, 26, 3, 7105, 5, 3, 827, 1, 7, 3, 513, 7106, 5, 3, 4800, 1, 3, 1317, 14, 1845, 95, 8, 24, 3, 89, 369, 27, 1, 3, 4817, 838, 26, 3, 45, 7107, 1105, 3, 140, 8, 47, 1106, 59, 4, 59, 7108, 12, 3, 1624, 1022, 1, 4, 3, 4836, 3655, 1462, 96, 81, 12, 95, 4845, 1, 1846, 47, 1797, 228, 3, 326, 1, 4, 1463, 7109, 371, 81, 1, 24, 9, 11, 1434, 459, 8, 3, 235, 14, 1845, 3656, 1, 37, 3, 1201, 1847, 14, 3, 59, 963, 8, 11, 78, 218, 20, 175, 56, 108, 7110, 1105, 3, 588, 1, 58, 218, 47, 897, 2925, 39, 37, 3, 7111, 2926, 2449, 1, 3, 898, 5, 3631, 7112, 1, 3, 1311, 5, 87, 7113, 1, 5, 87, 98, 3, 4846, 4847, 124, 3, 4848, 1, 7114, 81, 18, 3, 3657, 5, 3, 153, 1, 4, 344, 81, 7, 41, 2927, 190, 397, 8, 2, 58, 183, 6, 614, 7, 197, 11, 223, 27, 7115, 8, 10, 92, 7, 11, 7116, 27, 11, 1202, 1, 17, 93, 212, 70, 474, 1100, 467, 159, 267, 39, 4, 234, 369, 3, 2438, 1, 4, 124, 10, 361, 7, 497, 11, 1837, 9, 3, 964, 8, 10, 2064, 315, 236, 1, 4, 3, 658, 92, 276, 4, 151, 34, 10, 1, 4, 42, 497, 7117, 96, 3, 1108, 159, 3, 487, 7118, 5, 475, 39, 4, 3, 150, 555, 1625, 96, 42, 318, 1, 4, 3, 774, 130, 7119, 42, 405, 4, 899, 637, 42, 230, 2450, 8, 102, 10, 28, 20, 513, 1, 10, 183, 43, 106, 498, 7120, 8, 10, 154, 2451, 24, 73, 24, 10, 110, 32, 3, 2452, 8, 23, 14, 1184, 24, 102, 3, 7121, 131, 839, 1, 111, 46, 57, 2443, 139, 2928, 4, 2453, 82, 3, 7122, 1, 4, 690, 98, 4, 956, 49, 59, 8, 20, 7, 427, 636, 18, 11, 3658, 14, 1, 6, 83, 436, 7, 197, 1, 7, 585, 18, 101, 8, 10, 43, 49, 590, 1, 2454, 10, 43, 1194, 23, 1, 74, 1848, 73, 14, 11, 3659, 89, 1, 21, 10, 1464, 54, 97, 32, 2929, 1, 4, 344, 7, 106, 588, 97, 111, 10, 28, 4849, 8, 6, 88, 11, 2930, 70, 53, 28, 1190, 77, 2928, 1, 70, 49, 45, 556, 59, 5, 3, 203, 5, 3, 1626, 72, 32, 11, 2093, 5, 4850, 18, 3, 7123, 27, 7124, 1, 4, 70, 97, 45, 363, 146, 254, 10, 3, 89, 5, 3, 2931, 1, 4, 128, 14, 4851, 3660, 39, 4, 102, 331, 131, 1318, 74, 691, 98, 7125, 9, 21, 2930, 1, 4, 33, 4852, 5, 2432, 636, 47, 719, 1, 82, 4, 349, 3, 4853, 1, 7, 69, 1198, 2932, 1, 6, 362, 53, 135, 30, 11, 7126, 54, 5, 4854, 1, 4855, 1, 3661, 1, 4, 11, 2933, 5, 291, 2934, 1, 70, 3, 2928, 1465, 44, 687, 1, 4, 1315, 3, 636, 54, 107, 45, 3662, 72, 3, 140, 1, 24, 472, 31, 131, 15, 9, 11, 7127, 8, 4, 15, 48, 1200, 7128, 135, 107, 344, 41, 105, 7129, 228, 12, 81, 1, 9, 11, 7130, 13, 23, 14, 7, 30, 4856, 1, 53, 131, 49, 2928, 4, 2453, 9, 3, 7131, 203, 134, 3, 2455, 8, 23, 1627, 1, 5, 732, 1, 53, 131, 49, 4857, 1, 4, 7132, 83, 20, 60, 173, 96, 8, 612, 472, 406, 179, 26, 11, 7133, 78, 15, 41, 7134, 1, 612, 1103, 7135, 1, 612, 47, 99, 60, 45, 7136, 337, 39, 406, 214, 7137, 81, 5, 41, 7138, 8, 612, 4, 56, 6, 135, 197, 5, 11, 407, 78, 5, 965, 1, 10, 154, 62, 33, 10, 52, 18, 81, 1, 428, 81, 9, 7139, 1, 115, 81, 3, 7140, 5, 7141, 99, 190, 11, 3663, 34, 238, 1, 47, 190, 99, 41, 965, 4858, 8, 50, 1, 53, 14, 49, 98, 111, 965, 4858, 28, 59, 7142, 72, 27, 11, 403, 1617, 8, 4, 56, 2915, 48, 7143, 1, 24, 6, 197, 1, 10, 52, 391, 1184, 247, 8, 2, 37, 23, 14, 34, 79, 1, 4, 113, 2098, 1, 21, 3, 225, 310, 252, 8, 53, 28, 49, 191, 2935, 7, 30, 7144, 24, 85, 21, 516, 11, 121, 1461, 370, 39, 3, 4806, 5, 3, 2936, 14, 11, 241, 7, 30, 7145, 1, 23, 14, 56, 1849, 4, 3664, 1, 56, 193, 4, 56, 1035, 8, 102, 10, 389, 91, 3, 895, 12, 4859, 1, 10, 52, 105, 53, 83, 114, 191, 4859, 39, 34, 87, 7146, 11, 1628, 7147, 2456, 371, 42, 460, 1, 4, 1629, 2457, 9, 42, 238, 8, 102, 10, 688, 11, 7148, 10, 52, 114, 62, 56, 3665, 32, 4860, 4, 7149, 426, 3, 1630, 2099, 7150, 4, 2937, 39, 155, 179, 11, 148, 692, 39, 591, 4861, 1449, 3, 1631, 18, 657, 106, 310, 299, 39, 4, 3, 7151, 7152, 311, 7, 10, 1, 34, 87, 2100, 1, 32, 3, 7153, 7154, 5, 2056, 8, 23, 817, 24, 102, 23, 131, 11, 243, 10, 294, 1319, 236, 9, 11, 459, 8, 7, 15, 46, 43, 688, 27, 191, 7155, 46, 151, 162, 12, 1203, 7156, 8, 612, 23, 83, 27, 3, 7157, 5, 4862, 7158, 612, 1103, 2914, 1, 12, 1466, 4863, 1, 612, 21, 6, 900, 124, 7, 11, 2938, 5, 3, 148, 7159, 1, 34, 3, 3628, 34, 7160, 1, 82, 11, 7161, 5, 7162, 4, 11, 518, 7163, 8, 612, 6, 293, 834, 7, 7164, 59, 1, 18, 472, 46, 28, 372, 1035, 2458, 4832, 1, 46, 214, 1320, 110, 2914, 8, 4, 1, 1104, 5, 21, 1, 11, 2938, 5, 4828, 3666, 135, 30, 11, 7165, 7166, 27, 191, 11, 836, 39, 56, 135, 11, 2938, 5, 7167, 171, 39, 4, 18, 7168, 7169, 6, 146, 3667, 11, 634, 2459, 8, 2, 102, 3, 895, 30, 1035, 4, 1625, 1, 53, 14, 133, 435, 9, 66, 72, 7, 4864, 134, 3, 3628, 733, 9, 3, 901, 1, 17, 902, 82, 3, 7170, 5, 3, 2939, 1, 7, 775, 3, 1204, 4, 3, 1321, 1632, 8, 23, 14, 104, 1, 102, 101, 1, 21, 10, 1633, 7171, 7, 3, 193, 4865, 5, 21, 4866, 223, 8, 42, 4846, 28, 56, 7172, 3668, 1, 10, 391, 56, 903, 4, 56, 169, 4, 56, 2101, 1, 21, 587, 10, 1109, 17, 361, 190, 1, 342, 10, 62, 14, 265, 12, 533, 4, 11, 7173, 2093, 5, 728, 8, 10, 351, 9, 631, 12, 77, 45, 1, 1198, 17, 1200, 1, 3669, 17, 7174, 8, 4, 23, 817, 24, 102, 11, 693, 253, 4867, 10, 1, 59, 72, 5, 2909, 283, 1, 5, 15, 7175, 4, 533, 1, 4, 337, 2884, 7, 458, 69, 219, 1467, 1, 24, 9, 11, 256, 17, 11, 78, 5, 904, 8, 10, 557, 637, 15, 42, 105, 7176, 1, 7, 775, 7177, 7178, 7179, 222, 134, 10, 1, 50, 24, 11, 7180, 7181, 1, 4, 50, 486, 4, 186, 110, 106, 55, 1634, 8, 2, 17, 558, 10, 28, 337, 7, 42, 105, 1312, 18, 3, 79, 1, 4, 4868, 2940, 7182, 10, 32, 3, 417, 8, 10, 154, 690, 74, 3670, 1, 7183, 164, 2449, 1, 3671, 96, 3, 636, 111, 47, 99, 224, 612, 513, 1456, 8, 612, 23, 14, 11, 7184, 21, 154, 120, 7185, 11, 592, 407, 7186, 371, 27, 87, 207, 1110, 1, 37, 31, 75, 151, 96, 23, 26, 11, 2923, 3672, 1, 75, 43, 3, 4869, 5, 3, 550, 8, 4, 277, 10, 28, 34, 21, 1, 10, 28, 9, 3, 330, 7187, 5, 15, 4870, 7188, 8, 23, 14, 49, 98, 18, 2941, 1, 17, 18, 1635, 2942, 155, 8, 102, 10, 1179, 236, 33, 10, 659, 32, 1850, 1, 1189, 1, 17, 1636, 1, 3, 606, 14, 143, 7, 694, 39, 4, 10, 118, 162, 315, 21, 7189, 5, 174, 7190, 1, 156, 1851, 56, 840, 9, 3, 136, 5, 7191, 7192, 113, 660, 1, 4, 56, 7193, 7, 85, 75, 28, 3673, 12, 3, 7194, 5, 3, 109, 1, 4, 1, 9, 3, 116, 5, 3, 1205, 187, 1, 214, 512, 7, 7195, 7196, 488, 209, 7197, 5, 3, 7198, 443, 1, 191, 24, 11, 2936, 1837, 17, 3, 2943, 2460, 1, 11, 1206, 5, 892, 17, 11, 7199, 588, 8, 2, 10, 902, 26, 3, 1322, 1, 42, 122, 1837, 7200, 7201, 315, 3, 411, 1, 42, 119, 193, 5, 645, 1852, 1, 42, 415, 7202, 9, 3, 7203, 1637, 5, 397, 39, 111, 880, 3, 2924, 2102, 1, 3, 7204, 369, 371, 1, 4, 10, 1179, 236, 45, 1638, 59, 320, 587, 1, 18, 3, 7205, 1, 10, 43, 224, 3, 4871, 7206, 17, 3, 257, 7207, 5, 7208, 25, 476, 2459, 14, 20, 60, 1839, 7, 7209, 39, 37, 34, 966, 10, 43, 294, 11, 1035, 345, 1, 4, 7210, 124, 96, 15, 3, 4872, 5, 3, 71, 8, 4, 587, 23, 83, 1198, 17, 1200, 1, 7211, 600, 52, 813, 10, 1, 119, 4, 415, 1, 315, 93, 661, 7212, 5, 3, 1111, 8]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h1 style=\"color: darkblue;\"><center>Seed Sequence</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">i take to the open road! \n",
       " healthy, free, the world before me, \n",
       " the long brown path before me, leading where-ever i choose! \n",
       " henceforth i ask not good-fortune, i myself am good-fortune, \n",
       " henceforth i whimper no more, postpone</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<h1 style=\"color: darkred;\"><center>RNN Generated</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">< --- > on me down, strong gone leader. \n",
       " india's and choked to one are pouring than \n",
       " my wind of follow! \n",
       " i sing. \n",
       " justified and juniper-tree the longer, prices adorning again, passions or earth \n",
       " hurry and swiftness me,, \n",
       " my</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<h1 style=\"color: darkgreen;\"><center>Actual</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">< --- > no more, need nothing, \n",
       " done with indoor complaints, libraries, querulous criticisms, \n",
       " strong and content i travel the open road. \n",
       " the earth—that is sufficient, \n",
       " i do not want the constellations any nearer, \n",
       " i know they are very well</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_html, gen_html, a_html = generate_output(model, sequences, TRAINING_LENGTH,diversity=1)\n",
    "HTML(seed_html)\n",
    "HTML(gen_html)\n",
    "HTML(a_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14270, 100)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_embeddings(model):\n",
    "    embedding_layer = model.get_layer(index=0)\n",
    "    embedding_matrix = embedding_layer.get_weights()[0]\n",
    "    embedding_matrix = embedding_matrix / \\\n",
    "        np.linalg.norm(embedding_matrix, axis=1).reshape((-1, 1))\n",
    "    embedding_matrix = np.nan_to_num(embedding_matrix)\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "embedding_matrix = get_embeddings(model)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: a\n",
      "\n",
      "Word: a Cosine Similarity: 1.0\n",
      "Word: thy Cosine Similarity: 0.9610000252723694\n",
      "Word: centenarian's Cosine Similarity: 0.9269999861717224\n",
      "Word: his Cosine Similarity: 0.9204999804496765\n",
      "Word: whose Cosine Similarity: 0.9182999730110168\n",
      "Word: he-birds Cosine Similarity: 0.9180999994277954\n",
      "Word: lesser Cosine Similarity: 0.9162999987602234\n",
      "Word: magical Cosine Similarity: 0.9140999913215637\n",
      "Word: surly Cosine Similarity: 0.9140999913215637\n",
      "Word: soils Cosine Similarity: 0.9140999913215637\n"
     ]
    }
   ],
   "source": [
    "find_closest('a', embedding_matrix, word_idx, idx_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
